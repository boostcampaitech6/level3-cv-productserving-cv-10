{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Pix2StructConfig,\n",
    "    Pix2StructProcessor,\n",
    "    Pix2StructForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'IMAGE_DIR': '/home/lym/level3-cv-productserving-cv-10/data_select/task3_infographic/images/',\n",
    "    'MAX_PATCHES': 1024,\n",
    "    # 'MODEL_NAME': 'ybelkada/pix2struct-base',\n",
    "    'MODEL_NAME': 'google/pix2struct-infographics-vqa-base',\n",
    "    # 'MODEL_NAME': \"google/pix2struct-infographics-vqa-base\",\n",
    "    'IMG_SIZE': (256, 256),\n",
    "    'MAX_LEN': 256,\n",
    "    'LR': 3e-5,\n",
    "    'NB_EPOCHS': 10,\n",
    "    'TRAIN_BS': 6,\n",
    "    'VALID_BS': 2,\n",
    "    'ALL_SAMPLES': int(1e+100),\n",
    "    '_wandb_kernel': 'tanaym',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cl9xwlm8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd1e73d5ace45199e03aeda89da4344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.028 MB of 0.028 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-dust-1</strong> at: <a href='https://wandb.ai/ai_tech_6th_cv_level1/pytorch/runs/cl9xwlm8' target=\"_blank\">https://wandb.ai/ai_tech_6th_cv_level1/pytorch/runs/cl9xwlm8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240318_160035-cl9xwlm8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cl9xwlm8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4492b72d9c1048c888e89d2825a4f614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111354157846007, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jjh/level3-cv-productserving-cv-10/lym/wandb/run-20240318_160312-teahqp16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai_tech_6th_cv_level1/pytorch/runs/teahqp16' target=\"_blank\">drawn-gorge-2</a></strong> to <a href='https://wandb.ai/ai_tech_6th_cv_level1/pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai_tech_6th_cv_level1/pytorch' target=\"_blank\">https://wandb.ai/ai_tech_6th_cv_level1/pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai_tech_6th_cv_level1/pytorch/runs/teahqp16' target=\"_blank\">https://wandb.ai/ai_tech_6th_cv_level1/pytorch/runs/teahqp16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wandb_log(**kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        wandb.log({k: v})\n",
    "\n",
    "# Start W&B logging\n",
    "# W&B Login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# wandb.login(key=wb_key)\n",
    "\n",
    "run = wandb.init(\n",
    "    project='pytorch',\n",
    "    config=Config,\n",
    "    group='multi_modal',\n",
    "    job_type='train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add chart types as special tokens and a special BOS token\n",
    "BOS_TOKEN = \"<|BOS|>\"\n",
    "X_START = \"<x_start>\"\n",
    "X_END = \"<x_end>\"\n",
    "Y_START = \"<y_start>\"\n",
    "Y_END = \"<y_end>\"\n",
    "\n",
    "new_tokens = [\n",
    "    \"<line>\",\n",
    "    \"<vertical_bar>\",\n",
    "    \"<scatter>\",\n",
    "    \"<dot>\",\n",
    "    \"<horizontal_bar>\",\n",
    "    X_START,\n",
    "    X_END,\n",
    "    Y_START,\n",
    "    Y_END,\n",
    "    BOS_TOKEN,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augments():\n",
    "    return A.Compose([\n",
    "        A.Resize(width=Config['IMG_SIZE'][0], height=Config['IMG_SIZE'][1]),\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0],\n",
    "            std=[1, 1, 1],\n",
    "            max_pixel_value=255,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeneTechDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, augments=None):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.augments = augments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        img_path = '/home/jjh/level3-cv-productserving-cv-10/data/images/'+ item['image_local_name']\n",
    "        # img_path = '/home/lym/level3-cv-productserving-cv-10/data_select/task3_infographic/images/'+ item['image_local_name']\n",
    "        image = np.array(Image.open(img_path))\n",
    "        # image = cv2.imread(item['image'])\n",
    "        if self.augments:\n",
    "            image = self.augments(image=image)['image']\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\", \n",
    "            add_special_tokens=True, \n",
    "            max_patches=Config['MAX_PATCHES']\n",
    "        )\n",
    "        \n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        encoding[\"text\"] = item['answers']\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(extra_tokens=new_tokens):\n",
    "    # processor = AutoProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "    processor = Pix2StructProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "    model = Pix2StructForConditionalGeneration.from_pretrained(Config['MODEL_NAME'])\n",
    "    processor.image_processor.size = {\n",
    "        \"height\": Config['IMG_SIZE'][0],\n",
    "        \"width\": Config['IMG_SIZE'][1],\n",
    "    }\n",
    "    processor.image_processor.is_vqa = False\n",
    "\n",
    "    processor.tokenizer.add_tokens(extra_tokens)\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "    \n",
    "    texts = [item[\"text\"][0] for item in batch]\n",
    "    text_inputs = processor(\n",
    "        text=texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True, \n",
    "        max_length=Config['MAX_LEN']\n",
    "    )\n",
    "    new_batch[\"labels\"] = text_inputs.input_ids\n",
    "    for item in batch:\n",
    "        new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "        new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "    new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "    new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, processor, train_loader, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    Trains the model on all batches for one epoch with NVIDIA's AMP\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with autocast():\n",
    "        prog_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for idx, batch in prog_bar:\n",
    "            # print(\"icx, batch->\", idx, batch.keys())\n",
    "            labels = batch.pop(\"labels\").to('cuda')\n",
    "            flattened_patches = batch.pop(\"flattened_patches\").to('cuda')\n",
    "            attention_mask = batch.pop(\"attention_mask\").to('cuda')\n",
    "            print(flattened_patches.shape)\n",
    "            # for i in labels:\n",
    "            #     print(\"labels ->\", processor.decode(i))\n",
    "            outputs = model(\n",
    "                flattened_patches=flattened_patches,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            # print(outputs.logits)\n",
    "            # print(\" -> \", [processor.decode(index) for index in outputs.decoder_input_ids[0]])\n",
    "            # for i in outputs:\n",
    "            #     print(i)\n",
    "                # print(processor.decode(outputs.logits.argmax(dim=-1)))\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "            wandb_log(train_step_loss=loss.item())\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss = avg_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    wandb_log(train_loss=avg_loss)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, processor, valid_loader):\n",
    "    \"\"\"\n",
    "    Validates the model on all batches (in val set) for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    for idx, batch in prog_bar:\n",
    "        labels = batch.pop(\"labels\").to('cuda')\n",
    "        flattened_patches = batch.pop(\"flattened_patches\").to('cuda')\n",
    "        attention_mask = batch.pop(\"attention_mask\").to('cuda')\n",
    "        \n",
    "        outputs = model(\n",
    "            flattened_patches=flattened_patches,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "        wandb_log(val_step_loss=loss.item())\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "        # predictions = outputs.logits.argmax(dim=-1)\n",
    "        # correct_predictions = (predictions == labels).sum().item()\n",
    "        # total_predictions += correct_predictions\n",
    "        \n",
    "    avg_loss = avg_loss / len(valid_loader)\n",
    "    # accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Average validation loss: {avg_loss:.4f}\")\n",
    "    # print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "    wandb_log(val_loss=avg_loss)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, processor, train_loader, valid_loader, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    A nice function that binds it all together and reminds me of Keras days from 2018 :)\n",
    "    \"\"\"\n",
    "    best_val_loss = int(1e+5)\n",
    "    # print('->->')\n",
    "    for epoch in range(Config['NB_EPOCHS']):\n",
    "        # print('->->->')\n",
    "        print(f\"{'='*20} Epoch: {epoch+1} / {Config['NB_EPOCHS']} {'='*20}\")\n",
    "        _ = train_one_epoch(model, processor, train_loader, optimizer, scaler)\n",
    "        val_avg_loss = valid_one_epoch(model, processor, valid_loader)\n",
    "        \n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            print(f\"Saving best model so far with loss: {best_val_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"pix2struct_base_benetech.pt\")\n",
    "    print(f\"Best model with val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training cell\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Read the processed JSON file\n",
    "#     with open(\"/home/lym/level3-cv-productserving-cv-10/data_select/task3_infographic/info_val_select.json\", \"r\") as fl:\n",
    "#         dataset = json.load(fl)['data']\n",
    "        \n",
    "#     # Shuffle the dataset and select however samples you want for training\n",
    "#     shuffle(dataset)  # 이거 나중에 풀자.\n",
    "#     dataset = dataset[:Config['ALL_SAMPLES']]\n",
    "    \n",
    "#     # We are splitting the data naively for now\n",
    "#     split = 0.90\n",
    "#     train_samples = int(len(dataset) * split)\n",
    "#     train_ds = dataset[:train_samples+1]\n",
    "#     valid_ds = dataset[train_samples:]\n",
    "    \n",
    "#     # Yeah all that\n",
    "#     processor, model = get_model()\n",
    "#     # check_point = 'pix2struct_base_benetech.pt'\n",
    "    \n",
    "#     # checkpoint = torch.load(check_point)\n",
    "#     # model.load_state_dict(torch.load(check_point))\n",
    "\n",
    "#     model.to('cuda')\n",
    "#     wandb.watch(model)\n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(), lr=Config['LR'])\n",
    "    \n",
    "#     # Load the data into Datasets and then make DataLoaders for training\n",
    "#     train_dataset = BeneTechDataset(train_ds, processor, augments=augments())\n",
    "#     train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=Config['TRAIN_BS'], collate_fn=collator) # sulffle true 주자\n",
    "    \n",
    "#     valid_dataset = BeneTechDataset(valid_ds, processor, augments=augments())\n",
    "#     valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=Config['VALID_BS'], collate_fn=collator)\n",
    "    \n",
    "#     nb_train_steps = int(train_samples / Config['TRAIN_BS'] * Config['NB_EPOCHS'])\n",
    "    \n",
    "#     # Print out the data sizes we are training on\n",
    "#     print(f\"Training on {len(train_ds)} samples, Validating on {len(valid_ds)} samples\")\n",
    "#     # Train the model now\n",
    "#     fit(\n",
    "#         model=model,\n",
    "#         processor=processor,\n",
    "#         train_loader=train_dataloader,\n",
    "#         valid_loader=valid_dataloader,\n",
    "#         optimizer=optimizer,\n",
    "#         scaler=GradScaler(),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50354. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 23946 samples, Validating on 2801 samples\n",
      "==================== Epoch: 1 / 10 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234377d633d34a40b90f98ebe320ab3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1024, 770])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 31.75 GiB total capacity; 24.51 GiB already allocated; 115.94 MiB free; 24.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, Validating on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model now\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m fit(\n\u001b[1;32m     48\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     49\u001b[0m     processor\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[1;32m     50\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     51\u001b[0m     valid_loader\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[1;32m     52\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     53\u001b[0m     scaler\u001b[38;5;241m=\u001b[39mGradScaler(),\n\u001b[1;32m     54\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, processor, train_loader, valid_loader, optimizer, scaler)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNB_EPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print('->->->')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNB_EPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     _ \u001b[38;5;241m=\u001b[39m train_one_epoch(model, processor, train_loader, optimizer, scaler)\n\u001b[1;32m     11\u001b[0m     val_avg_loss \u001b[38;5;241m=\u001b[39m valid_one_epoch(model, processor, valid_loader)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_avg_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, processor, train_loader, optimizer, scaler)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(flattened_patches\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# for i in labels:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     print(\"labels ->\", processor.decode(i))\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     22\u001b[0m     flattened_patches\u001b[38;5;241m=\u001b[39mflattened_patches,\n\u001b[1;32m     23\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     24\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(outputs.logits)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(\" -> \", [processor.decode(index) for index in outputs.decoder_input_ids[0]])\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# for i in outputs:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(i)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# print(processor.decode(outputs.logits.argmax(dim=-1)))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1724\u001b[0m, in \u001b[0;36mPix2StructForConditionalGeneration.forward\u001b[0;34m(self, flattened_patches, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, labels, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1724\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1725\u001b[0m         flattened_patches\u001b[38;5;241m=\u001b[39mflattened_patches,\n\u001b[1;32m   1726\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1727\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1728\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1729\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1730\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1733\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1734\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1735\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1736\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1737\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:639\u001b[0m, in \u001b[0;36mPix2StructVisionModel.forward\u001b[0;34m(self, flattened_patches, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    635\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    637\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(flattened_patches)\n\u001b[0;32m--> 639\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    640\u001b[0m     embedding_output,\n\u001b[1;32m    641\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    642\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    643\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    644\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    645\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    647\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    648\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:360\u001b[0m, in \u001b[0;36mPix2StructVisionEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    353\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    354\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    355\u001b[0m         hidden_states,\n\u001b[1;32m    356\u001b[0m         attention_mask,\n\u001b[1;32m    357\u001b[0m         layer_head_mask,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n\u001b[1;32m    362\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:299\u001b[0m, in \u001b[0;36mPix2StructVisionLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# in Pix2StructVision, layernorm is applied before self-attention\u001b[39;00m\n\u001b[1;32m    297\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_attention_layer_norm(hidden_states)\n\u001b[0;32m--> 299\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    300\u001b[0m     hidden_states,\n\u001b[1;32m    301\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    302\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    303\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    304\u001b[0m )\n\u001b[1;32m    305\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    306\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:224\u001b[0m, in \u001b[0;36mPix2StructVisionAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    221\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(scores, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(scores\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin))\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mtype_as(scores)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 31.75 GiB total capacity; 24.51 GiB already allocated; 115.94 MiB free; 24.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read the processed JSON file\n",
    "    with open(\"/home/jjh/level3-cv-productserving-cv-10/data/qas/infographicsVQA_train_v1.0.json\", \"r\") as fl:\n",
    "        train_dataset = json.load(fl)['data']\n",
    "    with open(\"/home/jjh/level3-cv-productserving-cv-10/data/qas/infographicsVQA_val_v1.0_withQT.json\", \"r\") as fl:\n",
    "        val_dataset = json.load(fl)['data']\n",
    "        \n",
    "    # Shuffle the dataset and select however samples you want for training\n",
    "    shuffle(train_dataset) \n",
    "    # dataset = dataset[:Config['ALL_SAMPLES']]\n",
    "    \n",
    "    # We are splitting the data naively for now\n",
    "    split = 0.90\n",
    "    # train_samples = int(len(dataset) * split)\n",
    "    # train_ds = dataset[:train_samples+1]\n",
    "    # valid_ds = dataset[train_samples:]\n",
    "    train_ds = train_dataset\n",
    "    valid_ds = val_dataset\n",
    "\n",
    "    # train_samples = int(len(train_dataset) * split)\n",
    "    # train_ds = train_dataset[:train_samples]\n",
    "    # valid_ds = train_dataset[train_samples:]\n",
    "\n",
    "    # Yeah all that\n",
    "    processor, model = get_model()\n",
    "    # check_point = 'pix2struct_base_benetech.pt'\n",
    "    \n",
    "    # checkpoint = torch.load(check_point)\n",
    "    # model.load_state_dict(torch.load(check_point))\n",
    "\n",
    "    model.to('cuda')\n",
    "    wandb.watch(model)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=Config['LR'])\n",
    "    \n",
    "    # Load the data into Datasets and then make DataLoaders for training\n",
    "    train_dataset = BeneTechDataset(train_ds, processor)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=Config['TRAIN_BS'], collate_fn=collator) \n",
    "    \n",
    "    valid_dataset = BeneTechDataset(valid_ds, processor)\n",
    "    valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=Config['VALID_BS'], collate_fn=collator)\n",
    "    \n",
    "    # nb_train_steps = int(train_samples / Config['TRAIN_BS'] * Config['NB_EPOCHS'])\n",
    "    \n",
    "    # Print out the data sizes we are training on\n",
    "    print(f\"Training on {len(train_ds)} samples, Validating on {len(valid_ds)} samples\")\n",
    "    # Train the model now\n",
    "    fit(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        train_loader=train_dataloader,\n",
    "        valid_loader=valid_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scaler=GradScaler(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

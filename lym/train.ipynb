{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lym/miniconda3/envs/layoutv3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Pix2StructConfig,\n",
    "    Pix2StructProcessor,\n",
    "    Pix2StructForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'IMAGE_DIR': '/home/lym/level3-cv-productserving-cv-10/data_select/task3_infographic/images/',\n",
    "    'MAX_PATCHES': 1024,\n",
    "    # 'MODEL_NAME': 'ybelkada/pix2struct-base',\n",
    "    'MODEL_NAME': 'google/pix2struct-base',\n",
    "    # 'MODEL_NAME': \"google/pix2struct-infographics-vqa-base\",\n",
    "    'IMG_SIZE': (256, 256),\n",
    "    'MAX_LEN': 256,\n",
    "    'LR': 3e-5,\n",
    "    'NB_EPOCHS': 10,\n",
    "    'TRAIN_BS': 6,\n",
    "    'VALID_BS': 2,\n",
    "    'ALL_SAMPLES': int(1e+100),\n",
    "    '_wandb_kernel': 'tanaym',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medwin-minlee\u001b[0m (\u001b[33mlevel2-cv-10-detection\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lym/level3-cv-productserving-cv-10/pix2struct/pix2struct/wandb/run-20240318_034435-72rr56u3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/level2-cv-10-detection/pytorch/runs/72rr56u3' target=\"_blank\">dark-smoke-55</a></strong> to <a href='https://wandb.ai/level2-cv-10-detection/pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/level2-cv-10-detection/pytorch' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/level2-cv-10-detection/pytorch/runs/72rr56u3' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/pytorch/runs/72rr56u3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wandb_log(**kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        wandb.log({k: v})\n",
    "\n",
    "# Start W&B logging\n",
    "# W&B Login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# wandb.login(key=wb_key)\n",
    "\n",
    "run = wandb.init(\n",
    "    project='pytorch',\n",
    "    config=Config,\n",
    "    group='multi_modal',\n",
    "    job_type='train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add chart types as special tokens and a special BOS token\n",
    "BOS_TOKEN = \"<|BOS|>\"\n",
    "X_START = \"<x_start>\"\n",
    "X_END = \"<x_end>\"\n",
    "Y_START = \"<y_start>\"\n",
    "Y_END = \"<y_end>\"\n",
    "\n",
    "new_tokens = [\n",
    "    \"<line>\",\n",
    "    \"<vertical_bar>\",\n",
    "    \"<scatter>\",\n",
    "    \"<dot>\",\n",
    "    \"<horizontal_bar>\",\n",
    "    X_START,\n",
    "    X_END,\n",
    "    Y_START,\n",
    "    Y_END,\n",
    "    BOS_TOKEN,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augments():\n",
    "    return A.Compose([\n",
    "        A.Resize(width=Config['IMG_SIZE'][0], height=Config['IMG_SIZE'][1]),\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0],\n",
    "            std=[1, 1, 1],\n",
    "            max_pixel_value=255,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeneTechDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, augments=None):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.augments = augments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        img_path = '/home/lym/level3-cv-productserving-cv-10/data/images/'+ item['image_local_name']\n",
    "        # img_path = '/home/lym/level3-cv-productserving-cv-10/data_select/task3_infographic/images/'+ item['image_local_name']\n",
    "        image = np.array(Image.open(img_path))\n",
    "        # image = cv2.imread(item['image'])\n",
    "        if self.augments:\n",
    "            image = self.augments(image=image)['image']\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\", \n",
    "            add_special_tokens=True, \n",
    "            max_patches=Config['MAX_PATCHES']\n",
    "        )\n",
    "        \n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        encoding[\"text\"] = item['answers']\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(extra_tokens=new_tokens):\n",
    "    # processor = AutoProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "    processor = Pix2StructProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "    model = Pix2StructForConditionalGeneration.from_pretrained(Config['MODEL_NAME'])\n",
    "    processor.image_processor.size = {\n",
    "        \"height\": Config['IMG_SIZE'][0],\n",
    "        \"width\": Config['IMG_SIZE'][1],\n",
    "    }\n",
    "    processor.image_processor.is_vqa = False\n",
    "\n",
    "    processor.tokenizer.add_tokens(extra_tokens)\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "    \n",
    "    texts = [item[\"text\"][0] for item in batch]\n",
    "    text_inputs = processor(\n",
    "        text=texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True, \n",
    "        max_length=Config['MAX_LEN']\n",
    "    )\n",
    "    new_batch[\"labels\"] = text_inputs.input_ids\n",
    "    for item in batch:\n",
    "        new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "        new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "    new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "    new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, processor, train_loader, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    Trains the model on all batches for one epoch with NVIDIA's AMP\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with autocast():\n",
    "        prog_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for idx, batch in prog_bar:\n",
    "            # print(\"icx, batch->\", idx, batch.keys())\n",
    "            labels = batch.pop(\"labels\").to('cuda')\n",
    "            flattened_patches = batch.pop(\"flattened_patches\").to('cuda')\n",
    "            attention_mask = batch.pop(\"attention_mask\").to('cuda')\n",
    "            # for i in labels:\n",
    "            #     print(\"labels ->\", processor.decode(i))\n",
    "            outputs = model(\n",
    "                flattened_patches=flattened_patches,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            # print(outputs.logits)\n",
    "            # print(\" -> \", [processor.decode(index) for index in outputs.decoder_input_ids[0]])\n",
    "            # for i in outputs:\n",
    "            #     print(i)\n",
    "                # print(processor.decode(outputs.logits.argmax(dim=-1)))\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "            wandb_log(train_step_loss=loss.item())\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss = avg_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    wandb_log(train_loss=avg_loss)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, processor, valid_loader):\n",
    "    \"\"\"\n",
    "    Validates the model on all batches (in val set) for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    for idx, batch in prog_bar:\n",
    "        labels = batch.pop(\"labels\").to('cuda')\n",
    "        flattened_patches = batch.pop(\"flattened_patches\").to('cuda')\n",
    "        attention_mask = batch.pop(\"attention_mask\").to('cuda')\n",
    "        \n",
    "        outputs = model(\n",
    "            flattened_patches=flattened_patches,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "        wandb_log(val_step_loss=loss.item())\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "        # predictions = outputs.logits.argmax(dim=-1)\n",
    "        # correct_predictions = (predictions == labels).sum().item()\n",
    "        # total_predictions += correct_predictions\n",
    "        \n",
    "    avg_loss = avg_loss / len(valid_loader)\n",
    "    # accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Average validation loss: {avg_loss:.4f}\")\n",
    "    # print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "    wandb_log(val_loss=avg_loss)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, processor, train_loader, valid_loader, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    A nice function that binds it all together and reminds me of Keras days from 2018 :)\n",
    "    \"\"\"\n",
    "    best_val_loss = int(1e+5)\n",
    "    # print('->->')\n",
    "    for epoch in range(Config['NB_EPOCHS']):\n",
    "        # print('->->->')\n",
    "        print(f\"{'='*20} Epoch: {epoch+1} / {Config['NB_EPOCHS']} {'='*20}\")\n",
    "        _ = train_one_epoch(model, processor, train_loader, optimizer, scaler)\n",
    "        val_avg_loss = valid_one_epoch(model, processor, valid_loader)\n",
    "        \n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            print(f\"Saving best model so far with loss: {best_val_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"pix2struct_base_benetech.pt\")\n",
    "    print(f\"Best model with val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training cell\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Read the processed JSON file\n",
    "#     with open(\"/home/lym/level3-cv-productserving-cv-10/data_select/task3_infographic/info_val_select.json\", \"r\") as fl:\n",
    "#         dataset = json.load(fl)['data']\n",
    "        \n",
    "#     # Shuffle the dataset and select however samples you want for training\n",
    "#     shuffle(dataset)  # 이거 나중에 풀자.\n",
    "#     dataset = dataset[:Config['ALL_SAMPLES']]\n",
    "    \n",
    "#     # We are splitting the data naively for now\n",
    "#     split = 0.90\n",
    "#     train_samples = int(len(dataset) * split)\n",
    "#     train_ds = dataset[:train_samples+1]\n",
    "#     valid_ds = dataset[train_samples:]\n",
    "    \n",
    "#     # Yeah all that\n",
    "#     processor, model = get_model()\n",
    "#     # check_point = 'pix2struct_base_benetech.pt'\n",
    "    \n",
    "#     # checkpoint = torch.load(check_point)\n",
    "#     # model.load_state_dict(torch.load(check_point))\n",
    "\n",
    "#     model.to('cuda')\n",
    "#     wandb.watch(model)\n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(), lr=Config['LR'])\n",
    "    \n",
    "#     # Load the data into Datasets and then make DataLoaders for training\n",
    "#     train_dataset = BeneTechDataset(train_ds, processor, augments=augments())\n",
    "#     train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=Config['TRAIN_BS'], collate_fn=collator) # sulffle true 주자\n",
    "    \n",
    "#     valid_dataset = BeneTechDataset(valid_ds, processor, augments=augments())\n",
    "#     valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=Config['VALID_BS'], collate_fn=collator)\n",
    "    \n",
    "#     nb_train_steps = int(train_samples / Config['TRAIN_BS'] * Config['NB_EPOCHS'])\n",
    "    \n",
    "#     # Print out the data sizes we are training on\n",
    "#     print(f\"Training on {len(train_ds)} samples, Validating on {len(valid_ds)} samples\")\n",
    "#     # Train the model now\n",
    "#     fit(\n",
    "#         model=model,\n",
    "#         processor=processor,\n",
    "#         train_loader=train_dataloader,\n",
    "#         valid_loader=valid_dataloader,\n",
    "#         optimizer=optimizer,\n",
    "#         scaler=GradScaler(),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 200 samples, Validating on 200 samples\n",
      "==================== Epoch: 1 / 10 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 54.4121: 100%|██████████| 34/34 [00:26<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 52.3381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 23.5324: 100%|██████████| 100/100 [00:23<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 23.8940\n",
      "Saving best model so far with loss: 23.8940\n",
      "==================== Epoch: 2 / 10 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 28.3365: 100%|██████████| 34/34 [00:26<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 27.3934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 14.3565: 100%|██████████| 100/100 [00:24<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 14.8625\n",
      "Saving best model so far with loss: 14.8625\n",
      "==================== Epoch: 3 / 10 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 15.8629: 100%|██████████| 34/34 [00:26<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 16.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 7.1143:  31%|███       | 31/100 [00:08<00:19,  3.59it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:182\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 182\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:141\u001b[0m, in \u001b[0;36mBatchFeature._get_is_as_tensor_fns.<locals>.as_tensor\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    140\u001b[0m     value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(value)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, Validating on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model now\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGradScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, processor, train_loader, valid_loader, optimizer, scaler)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNB_EPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m _ \u001b[38;5;241m=\u001b[39m train_one_epoch(model, processor, train_loader, optimizer, scaler)\n\u001b[0;32m---> 11\u001b[0m val_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_avg_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n\u001b[1;32m     14\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m val_avg_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 56\u001b[0m, in \u001b[0;36mvalid_one_epoch\u001b[0;34m(model, processor, valid_loader)\u001b[0m\n\u001b[1;32m     53\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m prog_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(valid_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(valid_loader))\n\u001b[0;32m---> 56\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprog_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflattened_patches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflattened_patches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mBeneTechDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugments:\n\u001b[1;32m     17\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugments(image\u001b[38;5;241m=\u001b[39mimage)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAX_PATCHES\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m encoding \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m encoding\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     26\u001b[0m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/models/pix2struct/processing_pix2struct.py:104\u001b[0m, in \u001b[0;36mPix2StructProcessor.__call__\u001b[0;34m(self, images, text, add_special_tokens, padding, truncation, max_length, max_patches, stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_token_type_ids, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_encoding\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mis_vqa:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# add pixel_values\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# add pixel_values and bbox\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(\n\u001b[1;32m    110\u001b[0m         images, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, max_patches\u001b[38;5;241m=\u001b[39mmax_patches, header_text\u001b[38;5;241m=\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    111\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py:456\u001b[0m, in \u001b[0;36mPix2StructImageProcessor.preprocess\u001b[0;34m(self, images, header_text, do_convert_rgb, do_normalize, max_patches, patch_size, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# create attention mask in numpy\u001b[39;00m\n\u001b[1;32m    454\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m [(image\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 456\u001b[0m encoded_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchFeature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflattened_patches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:78\u001b[0m, in \u001b[0;36mBatchFeature.__init__\u001b[0;34m(self, data, tensor_type)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, tensor_type: Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m, TensorType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/layoutv3/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:188\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_values\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing values of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate padding \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read the processed JSON file\n",
    "    with open(\"/home/lym/level3-cv-productserving-cv-10/data/qas/infographicsVQA_train_v1.0_custom.json\", \"r\") as fl:\n",
    "        train_dataset = json.load(fl)['data']\n",
    "    with open(\"/home/lym/level3-cv-productserving-cv-10/data/qas/infographicsVQA_val_v1.0_withQT_custom.json\", \"r\") as fl:\n",
    "        val_dataset = json.load(fl)['data']\n",
    "        \n",
    "    # Shuffle the dataset and select however samples you want for training\n",
    "    shuffle(train_dataset) \n",
    "    # dataset = dataset[:Config['ALL_SAMPLES']]\n",
    "    \n",
    "    # We are splitting the data naively for now\n",
    "    split = 0.90\n",
    "    # train_samples = int(len(dataset) * split)\n",
    "    # train_ds = dataset[:train_samples+1]\n",
    "    # valid_ds = dataset[train_samples:]\n",
    "    train_ds = train_dataset\n",
    "    valid_ds = val_dataset\n",
    "\n",
    "    # train_samples = int(len(train_dataset) * split)\n",
    "    # train_ds = train_dataset[:train_samples]\n",
    "    # valid_ds = train_dataset[train_samples:]\n",
    "\n",
    "    # Yeah all that\n",
    "    processor, model = get_model()\n",
    "    # check_point = 'pix2struct_base_benetech.pt'\n",
    "    \n",
    "    # checkpoint = torch.load(check_point)\n",
    "    # model.load_state_dict(torch.load(check_point))\n",
    "\n",
    "    model.to('cuda')\n",
    "    wandb.watch(model)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=Config['LR'])\n",
    "    \n",
    "    # Load the data into Datasets and then make DataLoaders for training\n",
    "    train_dataset = BeneTechDataset(train_ds, processor, augments=augments())\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=Config['TRAIN_BS'], collate_fn=collator) \n",
    "    \n",
    "    valid_dataset = BeneTechDataset(valid_ds, processor, augments=augments())\n",
    "    valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=Config['VALID_BS'], collate_fn=collator)\n",
    "    \n",
    "    # nb_train_steps = int(train_samples / Config['TRAIN_BS'] * Config['NB_EPOCHS'])\n",
    "    \n",
    "    # Print out the data sizes we are training on\n",
    "    print(f\"Training on {len(train_ds)} samples, Validating on {len(valid_ds)} samples\")\n",
    "    # Train the model now\n",
    "    fit(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        train_loader=train_dataloader,\n",
    "        valid_loader=valid_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scaler=GradScaler(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

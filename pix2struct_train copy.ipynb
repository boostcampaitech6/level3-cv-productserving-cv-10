{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# import cv2\n",
    "# import glob\n",
    "# import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from transformers import (\n",
    "    # AutoProcessor,\n",
    "    # Pix2StructConfig,\n",
    "    Pix2StructProcessor,\n",
    "    Pix2StructForConditionalGeneration,\n",
    "    # get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'IMAGE_DIR': '/home/chy/level3-cv-productserving-cv-10/hy_info/task3/images/',\n",
    "    'MAX_PATCHES': 512,\n",
    "    \"patch_size\":{\n",
    "        \"height\" :16,\n",
    "        \"width\": 16\n",
    "    },\n",
    "    # 'MODEL_NAME': 'google/pix2struct-base',\n",
    "    'MODEL_NAME': \"google/pix2struct-infographics-vqa-base\",\n",
    "\n",
    "    'MAX_LEN': 256,\n",
    "    'LR': 3e-5,\n",
    "    'NB_EPOCHS': 10,\n",
    "    'TRAIN_BS': 4,\n",
    "    'VALID_BS': 2,\n",
    "    'ALL_SAMPLES': int(1e+100),\n",
    "    '_wandb_kernel': 'hy',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbigchoi3449\u001b[0m (\u001b[33mlevel2-cv-10-detection\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chy/level3-cv-productserving-cv-10/wandb/run-20240319_020805-cfrdll6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/level2-cv-10-detection/pytorch/runs/cfrdll6n' target=\"_blank\">super-wood-100</a></strong> to <a href='https://wandb.ai/level2-cv-10-detection/pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/level2-cv-10-detection/pytorch' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/level2-cv-10-detection/pytorch/runs/cfrdll6n' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/pytorch/runs/cfrdll6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wandb_log(**kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        wandb.log({k: v})\n",
    "\n",
    "# Start W&B logging\n",
    "# W&B Login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# wandb.login(key=wb_key)\n",
    "run = wandb.init(\n",
    "    project='pytorch',\n",
    "    config=Config,\n",
    "    group='multi_modal',\n",
    "    job_type='train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add chart types as special tokens and a special BOS token\n",
    "BOS_TOKEN = \"<|BOS|>\"\n",
    "X_START = \"<x_start>\"\n",
    "X_END = \"<x_end>\"\n",
    "Y_START = \"<y_start>\"\n",
    "Y_END = \"<y_end>\"\n",
    "\n",
    "new_tokens = [\n",
    "    \"<line>\",\n",
    "    \"<vertical_bar>\",\n",
    "    \"<scatter>\",\n",
    "    \"<dot>\",\n",
    "    \"<horizontal_bar>\",\n",
    "    X_START,\n",
    "    X_END,\n",
    "    Y_START,\n",
    "    Y_END,\n",
    "    BOS_TOKEN,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augments():\n",
    "    return A.Compose([\n",
    "        A.Resize(width=128, height=128),\n",
    "        A.Normalize(\n",
    "            mean=[0.68519514, 0.70161988, 0.68567898],\n",
    "            std=[0.35363774, 0.32057063, 0.32768584],\n",
    "            # Mean: [0.68519514 0.70161988 0.68567898]\n",
    "            # Std: [0.35363774 0.32057063 0.32768584]\n",
    "            # max_pixel_value=255,\n",
    "            max_pixel_value=1,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(extra_tokens=new_tokens):\n",
    "#     # processor = AutoProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "#     processor = Pix2StructProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "#     model = Pix2StructForConditionalGeneration.from_pretrained(Config['MODEL_NAME'])\n",
    "#     # processor.image_processor.size = {\n",
    "#     #     \"height\": Config['IMG_SIZE'][0],\n",
    "#     #     \"width\": Config['IMG_SIZE'][1],\n",
    "#     # }\n",
    "#     processor.image_processor.is_vqa = False\n",
    "\n",
    "#     processor.tokenizer.add_tokens(extra_tokens)\n",
    "#     model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "    \n",
    "#     return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "    print(new_batch)\n",
    "    texts = [item[\"text\"][0] for item in batch]\n",
    "    text_inputs = processor(\n",
    "        text=texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True, \n",
    "        max_length=Config['MAX_LEN']\n",
    "    )\n",
    "    new_batch[\"labels\"] = text_inputs.input_ids\n",
    "    for item in batch:\n",
    "        new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "        new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "    new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "    new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, processor, train_loader, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    Trains the model on all batches for one epoch with NVIDIA's AMP\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with autocast():\n",
    "        prog_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for idx, batch in prog_bar:\n",
    "            # print(\"icx, batch->\", idx, batch.keys())\n",
    "            labels = batch.pop(\"labels\").to('cuda')\n",
    "            flattened_patches = batch.pop(\"flattened_patches\").to('cuda')\n",
    "            attention_mask = batch.pop(\"attention_mask\").to('cuda')\n",
    "            # for i in labels:\n",
    "            #     print(\"labels ->\", processor.decode(i))\n",
    "            outputs = model(\n",
    "                flattened_patches=flattened_patches,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            # print(outputs.logits)\n",
    "            # print(\" -> \", [processor.decode(index) for index in outputs.decoder_input_ids[0]])\n",
    "            # for i in outputs:\n",
    "            #     print(i)\n",
    "                # print(processor.decode(outputs.logits.argmax(dim=-1)))\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "            wandb_log(train_step_loss=loss.item())\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss = avg_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    wandb_log(train_loss=avg_loss)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, processor, valid_loader):\n",
    "    \"\"\"\n",
    "    Validates the model on all batches (in val set) for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    for idx, batch in prog_bar:\n",
    "        labels = batch.pop(\"labels\").to('cuda')\n",
    "        flattened_patches = batch.pop(\"flattened_patches\").to('cuda')\n",
    "        attention_mask = batch.pop(\"attention_mask\").to('cuda')\n",
    "        \n",
    "        outputs = model(\n",
    "            flattened_patches=flattened_patches,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "        wandb_log(val_step_loss=loss.item())\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "        # predictions = outputs.logits.argmax(dim=-1)\n",
    "        # correct_predictions = (predictions == labels).sum().item()\n",
    "        # total_predictions += correct_predictions\n",
    "        \n",
    "    avg_loss = avg_loss / len(valid_loader)\n",
    "    # accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Average validation loss: {avg_loss:.4f}\")\n",
    "    # print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "    wandb_log(val_loss=avg_loss)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, processor, train_loader, valid_loader, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    A nice function that binds it all together and reminds me of Keras days from 2018 :)\n",
    "    \"\"\"\n",
    "    best_val_loss = int(1e+5)\n",
    "    # print('->->')\n",
    "    for epoch in range(Config['NB_EPOCHS']):\n",
    "        # print('->->->')\n",
    "        print(f\"{'='*20} Epoch: {epoch+1} / {Config['NB_EPOCHS']} {'='*20}\")\n",
    "        _ = train_one_epoch(model, processor, train_loader, optimizer, scaler)\n",
    "        val_avg_loss = valid_one_epoch(model, processor, valid_loader)\n",
    "        \n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            print(f\"Saving best model so far with loss: {best_val_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"pix2struct_base_benetech.pt\")\n",
    "    print(f\"Best model with val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the processed JSON file\n",
    "with open(\"/home/chy/level3-cv-productserving-cv-10/hy_info/task3/qas/infographicsVQA_train_v1.0.json\", \"r\") as f:\n",
    "    train_json = json.load(f)['data']\n",
    "with open(\"/home/chy/level3-cv-productserving-cv-10/hy_info/task3/qas/infographicsVQA_val_v1.0_withQT.json\", \"r\") as f:\n",
    "    valid_json = json.load(f)['data']\n",
    "with open(\"/home/chy/level3-cv-productserving-cv-10/hy_info/task3/qas/infographicsVQA_test_v1.0.json\", \"r\") as f:\n",
    "    test_json = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Pix2StructProcessor.from_pretrained(Config['MODEL_NAME'])\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(Config['MODEL_NAME'])\n",
    "\n",
    "processor.image_processor.is_vqa = False\n",
    "\n",
    "# processor.tokenizer.add_tokens()\n",
    "# model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# check_point = 'pix2struct_base_benetech.pt'\n",
    "\n",
    "# checkpoint = torch.load(check_point)\n",
    "# model.load_state_dict(torch.load(check_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class infographicsDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, augments=None):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.augments = augments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        img_path = '/home/chy/level3-cv-productserving-cv-10/hy_info/task3/images/'+ item['image_local_name']\n",
    "        image = np.array(Image.open(img_path))\n",
    "        if self.augments:\n",
    "            image = self.augments(image=image)['image']\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text = item['question'],\n",
    "            return_tensors=\"pt\", \n",
    "            # add_special_tokens=True, \n",
    "            # max_patches=Config['MAX_PATCHES']\n",
    "        )\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        encoding[\"Q\"] = item['question']\n",
    "        encoding[\"text\"] = item['answers'][0]\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "wandb.watch(model)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=Config['LR'])\n",
    "\n",
    "# Load the data into Datasets and then make DataLoaders for training\n",
    "train_dataset = infographicsDataset(train_json, processor, augments=augments())\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=Config['TRAIN_BS'], collate_fn=collator) # sulffle true 주자\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=Config['TRAIN_BS']) # sulffle true 주자\n",
    "\n",
    "valid_dataset = infographicsDataset(valid_json, processor, augments=augments())\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=Config['VALID_BS'], collate_fn=collator)\n",
    "\n",
    "# shuffle(train_dataset)\n",
    "# shuffle(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flattened_patches': tensor([[ 1.0000,  1.0000, -1.8885,  ..., -1.8746, -1.8906, -1.9911],\n",
       "         [ 1.0000,  2.0000, -1.8746,  ..., -1.8746, -1.8906, -1.9911],\n",
       "         [ 1.0000,  3.0000, -1.8746,  ..., -1.8853, -1.8866, -1.9834],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'attention_mask': tensor([1., 1., 1.,  ..., 0., 0., 0.]),\n",
       " 'decoder_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'decoder_input_ids': tensor([ 1609,   756,   280, 36637,  1450,   500,   689,  7287,   286,  4563,\n",
       "           946,   311,     1]),\n",
       " 'Q': 'Which type of fonts offer better readability in printed works?',\n",
       " 'text': 'serif fonts'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 23945 samples, Validating on 2801 samples\n",
      "==================== Epoch: 1 / 10 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4aa48648d74cc3a642860cccc2a3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flattened_patches': [], 'attention_mask': []}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 31.75 GiB total capacity; 30.71 GiB already allocated; 54.94 MiB free; 30.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, Validating on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model now\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m fit(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m     processor\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[1;32m      9\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     10\u001b[0m     valid_loader\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     12\u001b[0m     scaler\u001b[38;5;241m=\u001b[39mGradScaler(),\n\u001b[1;32m     13\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, processor, train_loader, valid_loader, optimizer, scaler)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNB_EPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print('->->->')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNB_EPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     _ \u001b[38;5;241m=\u001b[39m train_one_epoch(model, processor, train_loader, optimizer, scaler)\n\u001b[1;32m     11\u001b[0m     val_avg_loss \u001b[38;5;241m=\u001b[39m valid_one_epoch(model, processor, valid_loader)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_avg_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, processor, train_loader, optimizer, scaler)\u001b[0m\n\u001b[1;32m     17\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# for i in labels:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     print(\"labels ->\", processor.decode(i))\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     21\u001b[0m     flattened_patches\u001b[38;5;241m=\u001b[39mflattened_patches,\n\u001b[1;32m     22\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     23\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(outputs.logits)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(\" -> \", [processor.decode(index) for index in outputs.decoder_input_ids[0]])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# for i in outputs:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     print(i)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# print(processor.decode(outputs.logits.argmax(dim=-1)))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1737\u001b[0m, in \u001b[0;36mPix2StructForConditionalGeneration.forward\u001b[0;34m(self, flattened_patches, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, labels, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     decoder_attention_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1737\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1738\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1739\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1740\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1741\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1742\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1743\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1744\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1745\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1746\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1747\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1748\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1749\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1750\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1751\u001b[0m )\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1541\u001b[0m, in \u001b[0;36mPix2StructTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, labels, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[1;32m   1539\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m-> 1541\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Add last layer\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 31.75 GiB total capacity; 30.71 GiB already allocated; 54.94 MiB free; 30.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# nb_train_steps = int(train_samples / Config['TRAIN_BS'] * Config['NB_EPOCHS'])\n",
    "\n",
    "# Print out the data sizes we are training on\n",
    "print(f\"Training on {len(train_dataset)} samples, Validating on {len(valid_dataset)} samples\")\n",
    "# Train the model now\n",
    "fit(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    train_loader=train_dataloader,\n",
    "    valid_loader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scaler=GradScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

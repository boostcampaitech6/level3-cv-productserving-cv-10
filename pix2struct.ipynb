{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_info = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-infographics-vqa-base\").to(\"cuda\")\n",
    "processor_info = Pix2StructProcessor.from_pretrained(\"google/pix2struct-infographics-vqa-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3e6cc07048437d9cdaa62e43d2b20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d27558a89340c78288067a1840f2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3c8dc4cc1745c09c815984bd5eba65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/231 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47000217cbc4a85ac47f94b224a4fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d7dde690e64a74935f05a90ec4ad9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62d59375e33436eafa8abb4316e89dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06217fa2e01144f48b65a73a1d555d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-base\").to(\"cuda\")\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GenerationMixin.generate of Pix2StructForConditionalGeneration(\n",
       "  (encoder): Pix2StructVisionModel(\n",
       "    (embeddings): Pix2StructVisionEmbeddings(\n",
       "      (patch_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (row_embedder): Embedding(4096, 768)\n",
       "      (column_embedder): Embedding(4096, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Pix2StructVisionEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (1): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (2): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (3): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (4): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (5): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (6): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (7): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (8): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (9): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (10): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (11): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Pix2StructLayerNorm()\n",
       "  )\n",
       "  (decoder): Pix2StructTextModel(\n",
       "    (embed_tokens): Embedding(50244, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): Pix2StructLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (lm_head): Linear(in_features=768, out_features=50244, bias=False)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pix2StructForConditionalGeneration(\n",
       "  (encoder): Pix2StructVisionModel(\n",
       "    (embeddings): Pix2StructVisionEmbeddings(\n",
       "      (patch_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (row_embedder): Embedding(4096, 768)\n",
       "      (column_embedder): Embedding(4096, 768)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (encoder): Pix2StructVisionEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (1): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (2): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (3): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (4): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (5): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (6): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (7): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (8): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (9): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (10): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (11): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Pix2StructLayerNorm()\n",
       "  )\n",
       "  (decoder): Pix2StructTextModel(\n",
       "    (embed_tokens): Embedding(50244, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): Pix2StructLayerNorm()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (lm_head): Linear(in_features=768, out_features=50244, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pix2StructProcessor:\n",
       "- image_processor: Pix2StructImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"image_processor_type\": \"Pix2StructImageProcessor\",\n",
       "  \"is_vqa\": true,\n",
       "  \"max_patches\": 4096,\n",
       "  \"patch_size\": {\n",
       "    \"height\": 16,\n",
       "    \"width\": 16\n",
       "  },\n",
       "  \"processor_class\": \"Pix2StructProcessor\"\n",
       "}\n",
       "\n",
       "- tokenizer: T5TokenizerFast(name_or_path='google/pix2struct-infographics-vqa-large', vocab_size=50344, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50244: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50245: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50246: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50247: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50248: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50249: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50250: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50251: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50252: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50253: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50254: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50255: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50256: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50257: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50258: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50259: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50260: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50261: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50262: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50263: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50264: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50265: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50266: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50267: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50268: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50269: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50270: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50271: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50272: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50273: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50274: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50275: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50276: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50277: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50278: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50279: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50280: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50281: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50282: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50283: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50284: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50285: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50286: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50287: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50288: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50289: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50290: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50291: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50292: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50293: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50294: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50295: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50296: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50297: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50298: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50299: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50300: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50301: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50302: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50303: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50304: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50305: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50306: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50307: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50308: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50309: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50310: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50311: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50312: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50313: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50314: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50315: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50316: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50317: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50318: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50319: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50320: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50321: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50322: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50323: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50324: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50325: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50326: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50327: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50328: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50329: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50330: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50331: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50332: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50333: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50334: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50335: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50336: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50337: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50338: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50339: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50340: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50341: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50342: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50343: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Pix2StructProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답 :  what is the proper actions during a strong ground shaking? 100% of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chy/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"./hy_info/task3/images/10006.jpeg\")\n",
    "# question = \"how many days do they stay home who believes a contact is at high risk?\"\n",
    "question = 'what is the proper actions during a strong ground shaking?'\n",
    "inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "'''inference'''\n",
    "predictions = model.generate(**inputs)\n",
    "pred = processor.decode(predictions[0], skip_special_tokens=True)\n",
    "print('정답 : ', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 4221)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 770])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['flattened_patches'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchFeature.keys of {'flattened_patches': tensor([[[1.0000, 1.0000, 0.7582,  ..., 0.7566, 0.7566, 0.7566],\n",
       "         [1.0000, 2.0000, 0.7470,  ..., 0.7582, 0.7582, 0.7582],\n",
       "         [1.0000, 3.0000, 0.7489,  ..., 0.7531, 0.7531, 0.7531],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0'), 'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'decoder_input_ids': tensor([[ 361,  296,  276, 2051, 2109,  593,  283, 1139, 1235, 9817,  311,    1]],\n",
       "       device='cuda:0')}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GenerationMixin.generate of Pix2StructForConditionalGeneration(\n",
       "  (encoder): Pix2StructVisionModel(\n",
       "    (embeddings): Pix2StructVisionEmbeddings(\n",
       "      (patch_projection): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (row_embedder): Embedding(4096, 1536)\n",
       "      (column_embedder): Embedding(4096, 1536)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Pix2StructVisionEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (1): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (2): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (3): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (4): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (5): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (6): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (7): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (8): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (9): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (10): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (11): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (12): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (13): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (14): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (15): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (16): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (17): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Pix2StructLayerNorm()\n",
       "  )\n",
       "  (decoder): Pix2StructTextModel(\n",
       "    (embed_tokens): Embedding(50244, 1536)\n",
       "    (layer): ModuleList(\n",
       "      (0): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 24)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (value): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (output): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): Pix2StructLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (lm_head): Linear(in_features=1536, out_features=50244, bias=False)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "auto_processor = AutoProcessor.from_pretrained(\"google/pix2struct-infographics-vqa-large\")\n",
    "auto_processor.image_processor.is_vqa = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(\"/home/jjh/level3-cv-productserving-cv-10/data/images/\")\n",
    "with open(\"../data/qas/infographicsVQA_train_v1.0.json\") as f:\n",
    "    json_data = json.load(f)\n",
    "image_name = []\n",
    "q = []\n",
    "for d in json_data[\"data\"]:\n",
    "    image_name.append(d[\"image_local_name\"])\n",
    "    q.append(d[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2StructDataset(Dataset):\n",
    "    def __init__(self, image_dir, json_dir, processor, train):\n",
    "        self.img_dir = image_dir\n",
    "        with open(json_dir) as f:\n",
    "            self.json_data = json.load(f)\n",
    "        self.processor = processor\n",
    "        self.file_list = os.listdir(image_dir)\n",
    "        self.train = train\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        # image_name = self.file_list[index]\n",
    "        # img = Image.open(os.path.join(self.img_dir, image_name))\n",
    "\n",
    "        # for d in self.json_data['data']:\n",
    "        #     if d[\"image_local_name\"] == image_name:\n",
    "        #         q = d[\"question\"]\n",
    "        #         if self.train:\n",
    "        #             a = d[\"answers\"]\n",
    "        \n",
    "        # inputs = self.processor(images=img, text=q, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        # if self.train:\n",
    "        #     label = auto_processor(text=a, return_tensors=\"pt\").input_ids\n",
    "        #     inputs[\"labels\"] = label\n",
    "        data = self.json_data[\"data\"][index]\n",
    "        image_name = data[\"image_local_name\"]\n",
    "        img = Image.open(os.path.join(self.img_dir, image_name))\n",
    "        q = data[\"question\"]\n",
    "        inputs = self.processor(images=img, text=q, return_tensors=\"pt\").to('cuda')\n",
    "        if self.train:\n",
    "            a = data[\"answers\"]\n",
    "            # label = auto_processor(text=a, return_tensors=\"pt\").input_ids\n",
    "            label = auto_processor(text=a, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "            inputs[\"labels\"] = label\n",
    "        return inputs\n",
    "                    \n",
    "    def __len__(self): \n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/jjh/level3-cv-productserving-cv-10/data/images/'\n",
    "train_dataset = Pix2StructDataset(image_dir=img_dir, json_dir='../data/qas/infographicsVQA_train_v1.0.json', processor=processor, train=True)\n",
    "val_dataset = Pix2StructDataset(image_dir=img_dir, json_dir='../data/qas/infographicsVQA_val_v1.0_withQT.json', processor=processor, train=True)\n",
    "test_dataset = Pix2StructDataset(image_dir=img_dir, json_dir='../data/qas/infographicsVQA_test_v1.0.json', processor=processor, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 3\n",
    "# data = json_data[\"data\"][index]\n",
    "# image_name = data[\"image_local_name\"]\n",
    "# img = Image.open(os.path.join(img_dir, image_name))\n",
    "# q = data[\"question\"]\n",
    "# inputs = processor(images=img, text=q, return_tensors=\"pt\").to('cuda')\n",
    "# if  True:\n",
    "#     a = data[\"answers\"]\n",
    "#     label = auto_processor(text=a, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "#     inputs[\"labels\"] = label\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset)):\n\u001b[0;32m----> 2\u001b[0m     train_dataset[i]\n",
      "Cell \u001b[0;32mIn[35], line 29\u001b[0m, in \u001b[0;36mPix2StructDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     27\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, image_name))\n\u001b[1;32m     28\u001b[0m q \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(images\u001b[38;5;241m=\u001b[39mimg, text\u001b[38;5;241m=\u001b[39mq, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[1;32m     31\u001b[0m     a \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/processing_pix2struct.py:108\u001b[0m, in \u001b[0;36mPix2StructProcessor.__call__\u001b[0;34m(self, images, text, add_special_tokens, padding, truncation, max_length, max_patches, stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_token_type_ids, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(\n\u001b[1;32m    104\u001b[0m         images, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, max_patches\u001b[38;5;241m=\u001b[39mmax_patches, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# add pixel_values and bbox\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(\n\u001b[1;32m    109\u001b[0m         images, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, max_patches\u001b[38;5;241m=\u001b[39mmax_patches, header_text\u001b[38;5;241m=\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mis_vqa:\n\u001b[1;32m    113\u001b[0m     text_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    114\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    115\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    130\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/image_processing_utils.py:546\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    545\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py:458\u001b[0m, in \u001b[0;36mPix2StructImageProcessor.preprocess\u001b[0;34m(self, images, header_text, do_convert_rgb, do_normalize, max_patches, patch_size, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    453\u001b[0m         render_header(image, header_text[i], font_bytes\u001b[38;5;241m=\u001b[39mfont_bytes, font_path\u001b[38;5;241m=\u001b[39mfont_path)\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images)\n\u001b[1;32m    455\u001b[0m     ]\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 458\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image\u001b[38;5;241m=\u001b[39mimage, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# convert to torch tensor and permute\u001b[39;00m\n\u001b[1;32m    461\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_flattened_patches(\n\u001b[1;32m    463\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage, max_patches\u001b[38;5;241m=\u001b[39mmax_patches, patch_size\u001b[38;5;241m=\u001b[39mpatch_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format\n\u001b[1;32m    464\u001b[0m     )\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    466\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py:458\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    452\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    453\u001b[0m         render_header(image, header_text[i], font_bytes\u001b[38;5;241m=\u001b[39mfont_bytes, font_path\u001b[38;5;241m=\u001b[39mfont_path)\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images)\n\u001b[1;32m    455\u001b[0m     ]\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 458\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image\u001b[38;5;241m=\u001b[39mimage, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# convert to torch tensor and permute\u001b[39;00m\n\u001b[1;32m    461\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_flattened_patches(\n\u001b[1;32m    463\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage, max_patches\u001b[38;5;241m=\u001b[39mmax_patches, patch_size\u001b[38;5;241m=\u001b[39mpatch_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format\n\u001b[1;32m    464\u001b[0m     )\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    466\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py:353\u001b[0m, in \u001b[0;36mPix2StructImageProcessor.normalize\u001b[0;34m(self, image, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(image)\n\u001b[1;32m    351\u001b[0m adjusted_stddev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(std, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mprod(image\u001b[38;5;241m.\u001b[39mshape)))\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalize(\n\u001b[1;32m    354\u001b[0m     image,\n\u001b[1;32m    355\u001b[0m     mean\u001b[38;5;241m=\u001b[39mmean,\n\u001b[1;32m    356\u001b[0m     std\u001b[38;5;241m=\u001b[39madjusted_stddev,\n\u001b[1;32m    357\u001b[0m     data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m    358\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    360\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjh/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2046x770 and 768x1536)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m a \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(flattened_patches \u001b[38;5;241m=\u001b[39m f, attention_mask \u001b[38;5;241m=\u001b[39m a, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1724\u001b[0m, in \u001b[0;36mPix2StructForConditionalGeneration.forward\u001b[0;34m(self, flattened_patches, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, labels, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1724\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1725\u001b[0m         flattened_patches\u001b[38;5;241m=\u001b[39mflattened_patches,\n\u001b[1;32m   1726\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1727\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1728\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1729\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1730\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1733\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1734\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1735\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1736\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1737\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:637\u001b[0m, in \u001b[0;36mPix2StructVisionModel.forward\u001b[0;34m(self, flattened_patches, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    635\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 637\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(flattened_patches)\n\u001b[1;32m    639\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    640\u001b[0m     embedding_output,\n\u001b[1;32m    641\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    647\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:141\u001b[0m, in \u001b[0;36mPix2StructVisionEmbeddings.forward\u001b[0;34m(self, flattened_patches)\u001b[0m\n\u001b[1;32m    137\u001b[0m col_indices \u001b[38;5;241m=\u001b[39m flattened_patches[:, :, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    139\u001b[0m flattened_patches \u001b[38;5;241m=\u001b[39m flattened_patches[:, :, \u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m--> 141\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_projection(flattened_patches)\n\u001b[1;32m    142\u001b[0m row_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow_embedder(row_indices)\n\u001b[1;32m    143\u001b[0m col_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_embedder(col_indices)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2046x770 and 768x1536)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        f = batch[\"flattened_patches\"]\n",
    "        a = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model(flattened_patches = f, attention_mask = a, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        print(f\"{loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

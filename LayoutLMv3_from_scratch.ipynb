{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, random#, json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn # as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import editdistance\n",
    "import socket, datetime#, getpass\n",
    "import wandb as wb\n",
    "import transformers\n",
    "from transformers import get_scheduler\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFDocVQA(Dataset):\n",
    "\n",
    "    def __init__(self, imbd_dir, images_dir, split, kwargs):\n",
    "        data = np.load(os.path.join(imbd_dir, \"infographics_imdb_{:s}.npy\".format(split)), allow_pickle=True)\n",
    "        self.imdb = data\n",
    "        self.images_dir = images_dir\n",
    "        self.use_images = kwargs.get('use_images', False)\n",
    "        self.get_raw_ocr_data = kwargs.get('get_raw_ocr_data', False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imdb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.imdb[idx]\n",
    "        question = record['question']\n",
    "        context = ' '.join([word.lower() for word in record['ocr_tokens']])\n",
    "        \n",
    "        if 'answers' in record :\n",
    "            answers = list(set(answer.lower() for answer in record['answers']))\n",
    "        else : \n",
    "            answers = ['0' * len(question)] \n",
    "        \n",
    "        if self.use_images:\n",
    "            image_name = os.path.join(self.images_dir, \"{:s}\".format(record['image_name']))\n",
    "            image = Image.open(image_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.get_raw_ocr_data:\n",
    "            words = [word.lower() for word in record['ocr_tokens']]\n",
    "            context = ' '.join([word.lower() for word in record['ocr_tokens']])\n",
    "            boxes = np.array([bbox for bbox in record['ocr_normalized_boxes']])\n",
    "\n",
    "        start_idxs, end_idxs = self._get_start_end_idx(context, answers)\n",
    "\n",
    "        sample_info = {'question_id': record['question_id'],\n",
    "                       'questions': question,\n",
    "                       'contexts': context,\n",
    "                       'answers': answers,\n",
    "                       'start_indxs': start_idxs,\n",
    "                       'end_indxs': end_idxs\n",
    "                       }        \n",
    "\n",
    "        if self.use_images:\n",
    "            sample_info['image_names'] = image_name\n",
    "            sample_info['images'] = image\n",
    "        \n",
    "\n",
    "        if self.get_raw_ocr_data:\n",
    "            sample_info['words'] = words\n",
    "            sample_info['boxes'] = boxes\n",
    "\n",
    "        else:  # Information for extractive models\n",
    "            sample_info['start_indxs'] = start_idxs\n",
    "            sample_info['end_indxs'] = end_idxs\n",
    "\n",
    "        return sample_info\n",
    "\n",
    "    def _get_start_end_idx(self, context, answers):\n",
    "\n",
    "        answer_positions = []\n",
    "        for answer in answers:\n",
    "            start_idx = context.find(answer)\n",
    "\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(answer)\n",
    "                answer_positions.append([start_idx, end_idx])\n",
    "\n",
    "        if len(answer_positions) > 0:\n",
    "            start_idx, end_idx = random.choice(answer_positions)  # If both answers are in the context. Choose one randomly.\n",
    "        else:\n",
    "            start_idx, end_idx = 0, 0  # If the indices are out of the sequence length they are ignored. Therefore, we set them as a very big number.\n",
    "\n",
    "        return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, split):\n",
    "    # Specify special params for data processing depending on the model used.\n",
    "    dataset_kwargs = {}\n",
    "    dataset_kwargs['get_raw_ocr_data'] = True\n",
    "    dataset_kwargs['use_images'] = True\n",
    "\n",
    "    # Build dataset\n",
    "    dataset = IFDocVQA(config['imdb_dir'], config['images_dir'], split, dataset_kwargs)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.log_folder = config['save_dir']\n",
    "        experiment_date = datetime.datetime.now().strftime('%Y.%m.%d_%H.%M.%S')\n",
    "        self.experiment_name = \"{:s}__{:}\".format(config['model_name'], experiment_date)\n",
    "\n",
    "        dataset = config['dataset_name']\n",
    "        visual_encoder = config.get('visual_module', {}).get('model', '-').upper()\n",
    "\n",
    "        tags = [config['model_name'], dataset]\n",
    "        config = {'Model': config['model_name'], 'Weights': config['model_weights'], 'Dataset': dataset,\n",
    "                  'Visual Encoder': visual_encoder,\n",
    "                  'Batch size': config['batch_size'], 'Max. Seq. Length': config.get('max_sequence_length', '-'),\n",
    "                  'lr': config['lr'], 'seed': config['seed']}\n",
    "\n",
    "        self.logger = wb.init(project=\"Hyunyoung in the house motherfuckers~\", name=self.experiment_name, dir=self.log_folder, tags=tags, config=config)\n",
    "        self._print_config(config)\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.len_dataset = 0\n",
    "\n",
    "    def _print_config(self, config):\n",
    "        print(\"{:s}: {:s} \\n{{\".format(config['Model'], config['Weights']))\n",
    "        for k, v in config.items():\n",
    "            if k != 'Model' and k != 'Weights':\n",
    "                print(\"\\t{:}: {:}\".format(k, v))\n",
    "        print(\"}\\n\")\n",
    "\n",
    "    def log_model_parameters(self, model):\n",
    "        total_params = sum(p.numel() for p in model.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "\n",
    "        self.logger.config.update({\n",
    "            'Model Params': int(total_params / 1e6),  # In millions\n",
    "            'Model Trainable Params': int(trainable_params / 1e6)  # In millions\n",
    "        })\n",
    "\n",
    "        print(\"Model parameters: {:d} - Trainable: {:d} ({:2.2f}%)\".format(\n",
    "            total_params, trainable_params, trainable_params / total_params * 100))\n",
    "\n",
    "    def log_val_metrics(self, accuracy, anls, update_best=False):\n",
    "        str_msg = \"Epoch {:d}: Accuracy {:2.2f}%    ANLS {:2.4f}\".format(self.current_epoch, accuracy*100, anls)\n",
    "        self.logger.log({\n",
    "            'Val/Epoch Accuracy': accuracy,\n",
    "            'Val/Epoch ANLS': anls,\n",
    "        }, step=self.current_epoch*self.len_dataset + self.len_dataset)\n",
    "\n",
    "        if update_best:\n",
    "            str_msg += \"\\tBest Accuracy!\"\n",
    "            self.logger.config.update({\n",
    "                \"Best Accuracy\": accuracy,\n",
    "                \"Best epoch\": self.current_epoch\n",
    "            }, allow_val_change=True)\n",
    "\n",
    "        print(str_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, case_sensitive=False):\n",
    "\n",
    "        self.case_sensitive = case_sensitive\n",
    "        self.get_edit_distance = editdistance.eval\n",
    "        self.anls_threshold = 0.5\n",
    "\n",
    "        self.total_accuracies = []\n",
    "        self.total_anls = []\n",
    "\n",
    "        self.best_accuracy = 0\n",
    "        self.best_anls = 0\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def get_metrics(self, gt_answers, preds, answer_types=None, update_global_metrics=True):\n",
    "        answer_types = answer_types if answer_types is not None else ['string' for batch_idx in range(len(gt_answers))]\n",
    "        batch_accuracy = []\n",
    "        batch_anls = []\n",
    "        for batch_idx in range(len(preds)):\n",
    "            gt = [self._preprocess_str(gt_elm) for gt_elm in gt_answers[batch_idx]]\n",
    "            pred = self._preprocess_str(preds[batch_idx])\n",
    "\n",
    "            batch_accuracy.append(self._calculate_accuracy(gt, pred, answer_types[batch_idx]))\n",
    "            batch_anls.append(self._calculate_anls(gt, pred, answer_types[batch_idx]))\n",
    "\n",
    "        # if accumulate_metrics:\n",
    "        #     self.total_accuracies.extend(batch_accuracy)\n",
    "        #     self.total_anls.extend(batch_anls)\n",
    "\n",
    "        return {'accuracy': batch_accuracy, 'anls': batch_anls}\n",
    "\n",
    "    def get_retrieval_metric(self, gt_answer_page, pred_answer_page):\n",
    "        retrieval_precision = [1 if gt == pred else 0 for gt, pred in zip(gt_answer_page, pred_answer_page)]\n",
    "        return retrieval_precision\n",
    "\n",
    "    def update_global_metrics(self, accuracy, anls, current_epoch):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_epoch = current_epoch\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _preprocess_str(self, string):\n",
    "        if not self.case_sensitive:\n",
    "            string = string.lower()\n",
    "\n",
    "        return string.strip()\n",
    "\n",
    "    def _calculate_accuracy(self, gt, pred, answer_type):\n",
    "\n",
    "        if answer_type == 'not-answerable':\n",
    "            return 1 if pred in ['', 'none', 'NA', None, []] else 0\n",
    "\n",
    "        if pred == 'none' and answer_type != 'not-answerable':\n",
    "            return 0\n",
    "\n",
    "        for gt_elm in gt:\n",
    "            if gt_elm == pred:\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _calculate_anls(self, gt, pred, answer_type):\n",
    "        if len(pred) == 0:\n",
    "            return 0\n",
    "\n",
    "        if answer_type == 'not-answerable':\n",
    "            return 1 if pred in ['', 'none', 'NA', None, []] else 0\n",
    "\n",
    "        if pred == 'none' and answer_type != 'not-answerable':\n",
    "            return 0\n",
    "\n",
    "        answers_similarity = [1 - self.get_edit_distance(gt_elm, pred) / max(len(gt_elm), len(pred)) for gt_elm in gt]\n",
    "        max_similarity = max(answers_similarity)\n",
    "\n",
    "        anls = max_similarity if max_similarity >= self.anls_threshold else 0\n",
    "        return anls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Info_docvqa_collate_fn(batch):\n",
    "    batch = {k: [dic[k] for dic in batch] for k in batch[0]}  # List of dictionaries to dict of lists.\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, length_train_loader, config):\n",
    "    optimizer_class = getattr(transformers, 'AdamW')\n",
    "    optimizer = optimizer_class(model.model.parameters(), lr=float(config['lr']))\n",
    "    num_training_steps = config['train_epochs'] * length_train_loader\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=config['warmup_iterations'], num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, evaluator, **kwargs):\n",
    "\n",
    "    return_scores_by_sample = kwargs.get('return_scores_by_sample', False)\n",
    "    return_answers = kwargs.get('return_answers', False)\n",
    "\n",
    "    if return_scores_by_sample:\n",
    "        scores_by_samples = {}\n",
    "        total_accuracies = []\n",
    "        total_anls = []\n",
    "\n",
    "    else:\n",
    "        total_accuracies = 0\n",
    "        total_anls = 0\n",
    "\n",
    "    all_pred_answers = []\n",
    "    model.model.eval()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "        bs = len(batch['question_id'])\n",
    "        with torch.no_grad():\n",
    "            # outputs, pred_answers, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "            _, pred_answers, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "            print('real answer : ',batch['answers'])\n",
    "            print('predicted answer : ',pred_answers)\n",
    "\n",
    "        metric = evaluator.get_metrics(batch['answers'], pred_answers, batch.get('answer_type', None))\n",
    "\n",
    "        if return_scores_by_sample:\n",
    "            for batch_idx in range(bs):\n",
    "                scores_by_samples[batch['question_id'][batch_idx]] = {\n",
    "                    'accuracy': metric['accuracy'][batch_idx],\n",
    "                    'anls': metric['anls'][batch_idx],\n",
    "                    'pred_answer': pred_answers[batch_idx],\n",
    "                    'pred_answer_conf': answer_conf[batch_idx],\n",
    "                    'image_names' : batch['image_names'][batch_idx], # 여기서 부터 추가한 부분\n",
    "                    'question' : batch['questions'][batch_idx], \n",
    "                }\n",
    "\n",
    "        if return_scores_by_sample:\n",
    "            total_accuracies.extend(metric['accuracy'])\n",
    "            total_anls.extend(metric['anls'])\n",
    "\n",
    "        else:\n",
    "            total_accuracies += sum(metric['accuracy'])\n",
    "            total_anls += sum(metric['anls'])\n",
    "\n",
    "        if return_answers:\n",
    "            all_pred_answers.extend(pred_answers)\n",
    "\n",
    "    if not return_scores_by_sample:\n",
    "        total_accuracies = total_accuracies/len(data_loader.dataset)\n",
    "        total_anls = total_anls/len(data_loader.dataset)\n",
    "        scores_by_samples = []\n",
    "    return total_accuracies, total_anls, all_pred_answers, scores_by_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yaml(path, data):\n",
    "    # print(data)\n",
    "    with open(path, 'w+') as f:\n",
    "        yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, update_best=False, **kwargs):\n",
    "    save_dir = os.path.join(kwargs['save_dir'], 'checkpoints', \"{:s}_{:s}\".format(kwargs['model_name'].lower(), kwargs['dataset_name'].lower()))\n",
    "\n",
    "    tokenizer = model.tokenizer if hasattr(model, 'tokenizer') else model.processor if hasattr(model, 'processor') else None\n",
    "\n",
    "    if update_best:\n",
    "        model.model.save_pretrained(os.path.join(save_dir, \"best.ckpt\"))\n",
    "        tokenizer.save_pretrained(os.path.join(save_dir, \"best.ckpt\"))\n",
    "        save_yaml(os.path.join(save_dir, \"best.ckpt\", \"experiment_config.yml\"), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, **kwargs):\n",
    "\n",
    "    epochs = kwargs['train_epochs']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    seed_everything(kwargs['seed'])\n",
    "\n",
    "    evaluator = Evaluator(case_sensitive=False)\n",
    "    logger = Logger(config=kwargs)\n",
    "    logger.log_model_parameters(model)\n",
    "\n",
    "    train_dataset = build_dataset(config, 'train')\n",
    "    val_dataset   = build_dataset(config, 'val')\n",
    "    print('done')\n",
    "    # g = torch.Generator()\n",
    "    # g.manual_seed(kwargs['seed'])\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=Info_docvqa_collate_fn)\n",
    "    val_data_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=Info_docvqa_collate_fn)\n",
    "\n",
    "    logger.len_dataset = len(train_data_loader)\n",
    "    optimizer, lr_scheduler = build_optimizer(model, length_train_loader=len(train_data_loader), config=kwargs)\n",
    "\n",
    "    if kwargs.get('eval_start', False):\n",
    "        logger.current_epoch = -1\n",
    "        accuracy, anls, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)\n",
    "        is_updated = evaluator.update_global_metrics(accuracy, anls, -1)\n",
    "        logger.log_val_metrics(accuracy, anls, update_best=is_updated)\n",
    "\n",
    "    for epoch_ix in range(epochs):\n",
    "        logger.current_epoch = epoch_ix\n",
    "        train_epoch(train_data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs)\n",
    "        accuracy, anls, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)\n",
    "        is_updated = evaluator.update_global_metrics(accuracy, anls, epoch_ix)\n",
    "        logger.log_val_metrics(accuracy, anls, update_best=is_updated)\n",
    "        save_model(model, epoch_ix, update_best=is_updated, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs):\n",
    "    model.model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "        gt_answers = batch['answers']\n",
    "        outputs, pred_answers, _ = model.forward(batch, return_pred_answer=True)\n",
    "\n",
    "        loss = outputs.loss\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        metric = evaluator.get_metrics(gt_answers, pred_answers)\n",
    "\n",
    "        batch_acc = np.mean(metric['accuracy'])\n",
    "        batch_anls = np.mean(metric['anls'])\n",
    "\n",
    "        log_dict = {\n",
    "            'Train/Batch loss': outputs.loss.item(),\n",
    "            'Train/Batch Accuracy': batch_acc,         \n",
    "            'Train/Batch ANLS': batch_anls,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        logger.logger.log(log_dict, step=logger.current_epoch * logger.len_dataset + batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractive_confidence(outputs):\n",
    "    bs = len(outputs['start_logits'])\n",
    "    start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "    end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "    answ_confidence = []\n",
    "    for batch_idx in range(bs):\n",
    "        conf_mat = np.matmul(np.expand_dims(outputs.start_logits.softmax(dim=1)[batch_idx].unsqueeze(dim=0).detach().cpu(), -1),\n",
    "                             np.expand_dims(outputs.end_logits.softmax(dim=1)[batch_idx].unsqueeze(dim=0).detach().cpu(), 1)).squeeze(axis=0)\n",
    "\n",
    "        answ_confidence.append(\n",
    "            conf_mat[start_idxs[batch_idx], end_idxs[batch_idx]].item()\n",
    "        )\n",
    "\n",
    "    return answ_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutLMv3_hy:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.apply_ocr = config['apply_ocr']\n",
    "        self.processor = LayoutLMv3Processor.from_pretrained(config['model_weights'], apply_ocr=config['apply_ocr'])  # Check that this do not fuck up the code.\n",
    "        self.model = LayoutLMv3ForQuestionAnswering.from_pretrained(config['model_weights'])\n",
    "        self.ignore_index = 9999  # 0\n",
    "\n",
    "    # def parallelize(self):\n",
    "    #     self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, batch, return_pred_answer=False):\n",
    "\n",
    "        # bs = len(batch['question_id'])\n",
    "        question = batch['questions']\n",
    "        context = batch['contexts']\n",
    "        answers = batch['answers']\n",
    "        images = batch['images']\n",
    "\n",
    "        boxes = [(bbox * 1000).astype(int) for bbox in batch['boxes']]  # Scale boxes 0->1 to 0-->1000.\n",
    "        \n",
    "        if self.apply_ocr:\n",
    "            encoding = self.processor(images, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "        else:\n",
    "            encoding = self.processor(images, question, batch[\"words\"], boxes=boxes, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "\n",
    "        start_pos, end_pos = self.get_start_end_idx(encoding, context, answers)\n",
    "        outputs = self.model(**encoding, start_positions=start_pos, end_positions=end_pos)\n",
    "        pred_answers, answ_confidence = self.get_answer_from_model_output(encoding.input_ids, outputs) if return_pred_answer else None\n",
    "\n",
    "        return outputs, pred_answers, answ_confidence\n",
    "\n",
    "    def get_concat_v_multi_resize(self, im_list, resample=Image.BICUBIC):\n",
    "        min_width = min(im.width for im in im_list)\n",
    "        im_list_resize = [im.resize((min_width, int(im.height * min_width / im.width)), resample=resample) for im in im_list]\n",
    "\n",
    "        # Fix equal height for all images (breaks the aspect ratio).\n",
    "        heights = [im.height for im in im_list]\n",
    "        im_list_resize = [im.resize((im.height, max(heights)), resample=resample) for im in im_list_resize]\n",
    "\n",
    "        total_height = sum(im.height for im in im_list_resize)\n",
    "        dst = Image.new('RGB', (min_width, total_height))\n",
    "        pos_y = 0\n",
    "        for im in im_list_resize:\n",
    "            dst.paste(im, (0, pos_y))\n",
    "            pos_y += im.height\n",
    "        return dst\n",
    "\n",
    "    def get_start_end_idx(self, encoding, context, answers):\n",
    "        pos_idx = []\n",
    "        for batch_idx in range(len(encoding.input_ids)):\n",
    "            answer_pos = []\n",
    "            for answer in answers[batch_idx]:\n",
    "                encoded_answer = [token for token in self.processor.tokenizer.encode([answer], boxes=[0, 0, 0, 0]) if token not in self.processor.tokenizer.all_special_ids]\n",
    "                answer_tokens_length = len(encoded_answer)\n",
    "\n",
    "                for token_pos in range(len(encoding.input_ids[batch_idx])):\n",
    "                    if encoding.input_ids[batch_idx][token_pos: token_pos+answer_tokens_length].tolist() == encoded_answer:\n",
    "                        answer_pos.append([token_pos, token_pos + answer_tokens_length-1])\n",
    "\n",
    "            if len(answer_pos) == 0:\n",
    "                pos_idx.append([self.ignore_index, self.ignore_index])\n",
    "\n",
    "            else:\n",
    "                answer_pos = random.choice(answer_pos)  # To add variability, pick a random correct span.\n",
    "                pos_idx.append(answer_pos)\n",
    "\n",
    "        start_idxs = torch.LongTensor([idx[0] for idx in pos_idx]).to(self.model.device)\n",
    "        end_idxs = torch.LongTensor([idx[1] for idx in pos_idx]).to(self.model.device)\n",
    "\n",
    "        return start_idxs, end_idxs\n",
    "\n",
    "    def get_answer_from_model_output(self, input_tokens, outputs):\n",
    "        predicted_start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "        predicted_end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "        predicted_answers = [self.processor.tokenizer.decode(input_tokens[batch_idx][predicted_start_idxs[batch_idx]: predicted_end_idxs[batch_idx]+1], skip_special_tokens=True).strip() for batch_idx in range(len(input_tokens))]\n",
    "        # answers_conf = ((outputs.start_logits.max(dim=1).values + outputs.end_logits.max(dim=1).values) / 2).tolist()\n",
    "\n",
    "        start_logits = outputs.start_logits.softmax(dim=1).detach().cpu()\n",
    "        end_logits = outputs.end_logits.softmax(dim=1).detach().cpu()\n",
    "        answ_confidence = []\n",
    "        for batch_idx in range(len(input_tokens)):\n",
    "            conf_mat = np.matmul(np.expand_dims(start_logits[batch_idx].unsqueeze(dim=0), -1),\n",
    "                                 np.expand_dims(end_logits[batch_idx].unsqueeze(dim=0), 1)).squeeze(axis=0)\n",
    "\n",
    "            answ_confidence.append(\n",
    "                conf_mat[predicted_start_idxs[batch_idx], predicted_end_idxs[batch_idx]].item()\n",
    "            )\n",
    "\n",
    "        answ_confidence = get_extractive_confidence(outputs)\n",
    "\n",
    "        return predicted_answers, answ_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    # \"model\": \"hy\",\n",
    "    \"dataset\": \"infographics\",\n",
    "    \"eval_start\": True,\n",
    "    \"no_eval_start\": False,\n",
    "    \"page_retrieval\": None,\n",
    "    \"batch_size\": None,\n",
    "    \"max_sequence_length\": None,\n",
    "    \"seed\": 42,\n",
    "    \"save_dir\": \"saving_dir/\",\n",
    "    \"apply_ocr\": False,\n",
    "    \"data_parallel\": False,\n",
    "    \"no_data_parallel\": False,\n",
    "    \"model_name\": \"hy\",\n",
    "    \"model_weights\": \"microsoft/layoutlmv3-base\",\n",
    "    \"device\": \"cuda\",\n",
    "    # \"training_parameters\": {\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"train_epochs\": 10,\n",
    "    \"warmup_iterations\": 5,\n",
    "    # },\n",
    "    \"dataset_name\": \"infographicVQA\",\n",
    "    # \"imdb_dir\": \"./task3/imdb\",\n",
    "    # \"images_dir\": \"./task3/images\",\n",
    "    \"imdb_dir\": \"./hy_info/Task3_test/imdb\",\n",
    "    \"images_dir\": \"./hy_info/Task3_test/images\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['qa_outputs.dense.bias', 'qa_outputs.dense.weight', 'qa_outputs.out_proj.bias', 'qa_outputs.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbigchoi3449\u001b[0m (\u001b[33mlevel2-cv-10-detection\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>saving_dir/wandb/run-20240313_202053-vnbolp5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~/runs/vnbolp5b' target=\"_blank\">hy__2024.03.13_20.20.51</a></strong> to <a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~/runs/vnbolp5b' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~/runs/vnbolp5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hy: microsoft/layoutlmv3-base \n",
      "{\n",
      "\tDataset: infographicVQA\n",
      "\tVisual Encoder: -\n",
      "\tBatch size: 20\n",
      "\tMax. Seq. Length: None\n",
      "\tlr: 0.0001\n",
      "\tseed: 42\n",
      "}\n",
      "\n",
      "Model parameters: 125919106 - Trainable: 125919106 (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chy/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/chy/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:720: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.tensor(value)\n",
      "/home/chy/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 1/2 [00:04<00:04,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['pinterest'], ['restaurants, interior design, wedding venues'], ['linkedin, facebook'], ['linkedin'], ['facebook, twitter', 'twitter, facebook'], ['bakeries & coffee shops, travel agencies, art museums'], ['friendster'], ['linkedin'], ['2004'], ['youtube'], ['picaboo'], ['2003'], ['medium'], ['high'], ['facebook'], ['instagram'], ['2006'], ['500 thousand'], ['100'], ['linkedin']]\n",
      "predicted answer :  ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"2013! the facebook dislike button scam afthese attempted anstall malware 84% safe.we we're part of the problem not properly using or under -utilizing social\", \"2013! the facebook dislike button scam afthese attempted anstall malware 84% safe.we we're part of the problem not properly using or under -utilizing social\", \"2013! the facebook dislike button scam afthese attempted anstall malware 84% safe.we we're part of the problem not properly using or under -utilizing social\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['twitter'], ['red'], ['instagram'], ['audience, organization, offering', 'organization, offering, audience', 'offering, audience, organization'], ['benefits, features, elevator pitch', 'elevator pitch, benefits, features', 'features, elevator pitch, benefits'], ['mission statement, core values, culture', 'core values, culture, mission statement', 'culture, mission statement, core values'], ['40', '40%'], ['u.s.', 'the u.s.'], ['u.k.', 'the u.k.'], ['90%', '90'], ['facebook'], ['north dakota']]\n",
      "predicted answer :  [\"2013! the facebook dislike button scam afthese attempted anstall malware 84% safe.we we're part of the problem not properly using or under -utilizing social\", '', \"2013! the facebook dislike button scam afthese attempted anstall malware 84% safe.we we're part of the problem not properly using or under -utilizing social\", '', '', '', '', '', '', '', '', '']\n",
      "Epoch -1: Accuracy 0.00%    ANLS 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:04<00:00,  8.12s/it]\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['pinterest'], ['restaurants, interior design, wedding venues'], ['linkedin, facebook'], ['linkedin'], ['facebook, twitter', 'twitter, facebook'], ['bakeries & coffee shops, travel agencies, art museums'], ['friendster'], ['linkedin'], ['2004'], ['youtube'], ['picaboo'], ['2003'], ['medium'], ['high'], ['facebook'], ['instagram'], ['2006'], ['500 thousand'], ['100'], ['linkedin']]\n",
      "predicted answer :  ['$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', \"750 tweets per second! 2010 that's a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013 vine fine a 2014 uk's insta ads instagram brings sponsored life of a starman 2015 lived in vr\", '', '', '', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', '$$$ good for these business types: luxury goods software b2b companies & services providers bakeries & travel agencies art museums coffee shops f facebook', \"750 tweets per second! 2010 that's a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013 vine fine a 2014 uk's insta ads instagram brings sponsored life of a starman 2015 lived in vr\", '', \"7.2 media % of profiles social were fake in q1 2013! the facebook dislike button scam afthese attempted anstall malware 84% safe.we we're part of the problem not properly using or under -utilizing social media privacy settings 10% had posting 30% include thelr thelf\", '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['twitter'], ['red'], ['instagram'], ['audience, organization, offering', 'organization, offering, audience', 'offering, audience, organization'], ['benefits, features, elevator pitch', 'elevator pitch, benefits, features', 'features, elevator pitch, benefits'], ['mission statement, core values, culture', 'core values, culture, mission statement', 'culture, mission statement, core values'], ['40', '40%'], ['u.s.', 'the u.s.'], ['u.k.', 'the u.k.'], ['90%', '90'], ['facebook'], ['north dakota']]\n",
      "predicted answer :  ['', '', '', 'dbi digital branding institute www.digitalbrandinglnstitute.con', 'dbi digital branding institute www.digitalbrandinglnstitute.con', 'dbi digital branding institute www.digitalbrandinglnstitute.con', '', '', '', '', '', '']\n",
      "Epoch 0: Accuracy 0.00%    ANLS 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:06<00:00,  8.27s/it]\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['pinterest'], ['restaurants, interior design, wedding venues'], ['linkedin, facebook'], ['linkedin'], ['facebook, twitter', 'twitter, facebook'], ['bakeries & coffee shops, travel agencies, art museums'], ['friendster'], ['linkedin'], ['2004'], ['youtube'], ['picaboo'], ['2003'], ['medium'], ['high'], ['facebook'], ['instagram'], ['2006'], ['500 thousand'], ['100'], ['linkedin']]\n",
      "predicted answer :  ['', '', '', '', '', '', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004', '', '', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['twitter'], ['red'], ['instagram'], ['audience, organization, offering', 'organization, offering, audience', 'offering, audience, organization'], ['benefits, features, elevator pitch', 'elevator pitch, benefits, features', 'features, elevator pitch, benefits'], ['mission statement, core values, culture', 'core values, culture, mission statement', 'culture, mission statement, core values'], ['40', '40%'], ['u.s.', 'the u.s.'], ['u.k.', 'the u.k.'], ['90%', '90'], ['facebook'], ['north dakota']]\n",
      "predicted answer :  ['', '', '', '', '', '', '', 'facebook tagged show. 79% use the internet 47% 47% use use one social media site 65% twitter', 'facebook tagged show. 79% use the internet 47% 47% use use one social media site 65% twitter use blogger', '', '', '']\n",
      "Epoch 1: Accuracy 0.00%    ANLS 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:05<00:00,  8.21s/it]\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['pinterest'], ['restaurants, interior design, wedding venues'], ['linkedin, facebook'], ['linkedin'], ['facebook, twitter', 'twitter, facebook'], ['bakeries & coffee shops, travel agencies, art museums'], ['friendster'], ['linkedin'], ['2004'], ['youtube'], ['picaboo'], ['2003'], ['medium'], ['high'], ['facebook'], ['instagram'], ['2006'], ['500 thousand'], ['100'], ['linkedin']]\n",
      "predicted answer :  ['female', '', '', 'mortar', '', '', '', 'mortar', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004', '', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004 after f 2004 facebook begins facebook was launched b facebook', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004', '', '', 'brick', '', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['twitter'], ['red'], ['instagram'], ['audience, organization, offering', 'organization, offering, audience', 'offering, audience, organization'], ['benefits, features, elevator pitch', 'elevator pitch, benefits, features', 'features, elevator pitch, benefits'], ['mission statement, core values, culture', 'core values, culture, mission statement', 'culture, mission statement, core values'], ['40', '40%'], ['u.s.', 'the u.s.'], ['u.k.', 'the u.k.'], ['90%', '90'], ['facebook'], ['north dakota']]\n",
      "predicted answer :  ['', 'green, red, white', '', '', '', '', '', '', '', '65', '', '65']\n",
      "Epoch 2: Accuracy 0.00%    ANLS 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:05<00:00,  8.17s/it]\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['pinterest'], ['restaurants, interior design, wedding venues'], ['linkedin, facebook'], ['linkedin'], ['facebook, twitter', 'twitter, facebook'], ['bakeries & coffee shops, travel agencies, art museums'], ['friendster'], ['linkedin'], ['2004'], ['youtube'], ['picaboo'], ['2003'], ['medium'], ['high'], ['facebook'], ['instagram'], ['2006'], ['500 thousand'], ['100'], ['linkedin']]\n",
      "predicted answer :  ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2013! the facebook dislike button', '60 seconds? data in 2012--2013 breaches 2013', '2013! the facebook dislike button']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['twitter'], ['red'], ['instagram'], ['audience, organization, offering', 'organization, offering, audience', 'offering, audience, organization'], ['benefits, features, elevator pitch', 'elevator pitch, benefits, features', 'features, elevator pitch, benefits'], ['mission statement, core values, culture', 'core values, culture, mission statement', 'culture, mission statement, core values'], ['40', '40%'], ['u.s.', 'the u.s.'], ['u.k.', 'the u.k.'], ['90%', '90'], ['facebook'], ['north dakota']]\n",
      "predicted answer :  ['2013! the facebook dislike button', 'red, white', '', 'digital brand voice', 'digital', 'digital', '', '', 'royal wedding come from? americans are americans love followers television and nearly half of all americans they like others are now members of at least to know it one social network double 77% the proportion of just two that years ago. use social to share their love of a out of all american adults facebook tagged show. 79% use the internet 47% 47% use use one social media site 65% twitter', '', '', '']\n",
      "Epoch 3: Accuracy 0.00%    ANLS 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:05<00:00,  8.15s/it]\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['pinterest'], ['restaurants, interior design, wedding venues'], ['linkedin, facebook'], ['linkedin'], ['facebook, twitter', 'twitter, facebook'], ['bakeries & coffee shops, travel agencies, art museums'], ['friendster'], ['linkedin'], ['2004'], ['youtube'], ['picaboo'], ['2003'], ['medium'], ['high'], ['facebook'], ['instagram'], ['2006'], ['500 thousand'], ['100'], ['linkedin']]\n",
      "predicted answer :  ['', 'twitter', 'twitter', '', 'twitter', 'twitter', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004 after f 2004 facebook begins facebook was launched b facebook but birth of youtube 2005 ube a platform or 2005 the nike advert featuring facebo canada, october - 2006 myspace is booming myspace becams twitter launched on july 15th 750 tweets per second! 2010 that\\'s a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013 vine fine a 2014', '', 'sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004 after f 2004 facebook begins facebook was launched b facebook but birth of youtube 2005 ube a platform or 2005 the nike advert featuring facebo canada, october - 2006 myspace is booming myspace becams twitter launched on july 15th 750 tweets per second! 2010 that\\'s a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004 after f 2004 facebook begins facebook was launched b facebook but birth of youtube 2005 ube a platform or 2005 the nike advert featuring facebo canada, october - 2006 myspace is booming myspace becams twitter launched on july 15th 750 tweets per second! 2010 that\\'s a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013 vine fine a 2014', '1969 social media attempt (advanced research projects transmitting two had planned start of social sites 1997 profiles sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004', 'sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004 after f 2004 facebook begins facebook was launched b facebook but birth of youtube 2005 ube a platform or 2005 the nike advert featuring facebo canada, october - 2006 myspace is booming myspace becams twitter launched on july 15th 750 tweets per second! 2010 that\\'s a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013 vine fine a 2014', 'twitter', 'twitter', 'twitter', 'twitter', 'sixdegree grees\" creativity and 1999 status blogs f keeping facebook for their a challenger emerge$ 2002 blunders of link up with 2003 businesses gmail received 2004 after f 2004 facebook begins facebook was launched b facebook but birth of youtube 2005 ube a platform or 2005 the nike advert featuring facebo canada, october - 2006 myspace is booming myspace becams twitter launched on july 15th 750 tweets per second! 2010 that\\'s a day launch of r 2011 snapchat messaging short videos. mega views video hosting 2013', '', '2013', '2013']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real answer :  [['twitter'], ['red'], ['instagram'], ['audience, organization, offering', 'organization, offering, audience', 'offering, audience, organization'], ['benefits, features, elevator pitch', 'elevator pitch, benefits, features', 'features, elevator pitch, benefits'], ['mission statement, core values, culture', 'core values, culture, mission statement', 'culture, mission statement, core values'], ['40', '40%'], ['u.s.', 'the u.s.'], ['u.k.', 'the u.k.'], ['90%', '90'], ['facebook'], ['north dakota']]\n",
      "predicted answer :  ['2013', 'green, red, white', '', 'digital brand voice', 'digital brand voice demographics audience personas', 'digital brand voice demographics audience personas', '', '', '65% friends from from the he u.k americans want americans love americans to be distracted video games are hyper-social from reality video games are the second most social media now reaches the the more than 63 million active heavily -used internet activity, majority of americans', '', '', '']\n",
      "Epoch 4: Accuracy 0.00%    ANLS 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:36<00:36,  9.08s/it]"
     ]
    }
   ],
   "source": [
    "config = args_dict\n",
    "# config.pop('model')\n",
    "model_name = config['model_name'].lower()\n",
    "if 'save_dir' in config:\n",
    "    if not config['save_dir'].endswith('/'):\n",
    "        config['save_dir'] = config['save_dir'] + '/'\n",
    "\n",
    "    if not os.path.exists(config['save_dir']):\n",
    "        os.makedirs(config['save_dir'])\n",
    "\n",
    "# if 'seed' not in config:\n",
    "#     print(\"Seed not specified. Setting default seed to '{:d}'\".format(42))\n",
    "#     config['seed'] = 42\n",
    "\n",
    "model = LayoutLMv3_hy(config)\n",
    "\n",
    "if config['device'] == 'cuda' and config['data_parallel'] and torch.cuda.device_count() > 1:\n",
    "    model.parallelize()\n",
    "\n",
    "model.model.to(config['device'])\n",
    "\n",
    "train(model, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정리\n",
    "#1 이미지 받기\n",
    "\n",
    "#2 퀘스쳔 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

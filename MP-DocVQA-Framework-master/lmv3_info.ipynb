{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\n",
    "from PIL import Image\n",
    "import models._model_utils as model_utils\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutLMv3:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.processor = LayoutLMv3Processor.from_pretrained(config['model_weights'], apply_ocr=False)  # Check that this do not fuck up the code.\n",
    "        self.model = LayoutLMv3ForQuestionAnswering.from_pretrained(config['model_weights'])\n",
    "        self.page_retrieval = config['page_retrieval'].lower() if 'page_retrieval' in config else None\n",
    "        self.ignore_index = 9999  # 0\n",
    "\n",
    "        # img = Image.open('./data_select/task3_infographic/images/10022.jpeg')\n",
    "        # self.processor(img, 'question', ['words'], boxes=[[1, 2, 3, 4]])\n",
    "\n",
    "    def parallelize(self):\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, batch, return_pred_answer=False):\n",
    "\n",
    "        bs = len(batch['question_id'])\n",
    "        question = batch['questions']\n",
    "        context = batch['contexts']\n",
    "        answers = batch['answers']\n",
    "        images = batch['images']\n",
    "\n",
    "        if self.page_retrieval == 'logits':\n",
    "            outputs = []\n",
    "            pred_answers = []\n",
    "            pred_answer_pages = []\n",
    "            answ_confidence = []\n",
    "            for batch_idx in range(bs):\n",
    "                images = [Image.open(img_path).convert(\"RGB\") for img_path in batch['image_names'][batch_idx]]\n",
    "                boxes = [(bbox * 1000).astype(int) for bbox in batch['boxes'][batch_idx]]\n",
    "                document_encoding = self.processor(images, [question[batch_idx]] * len(images), batch[\"words\"][batch_idx], boxes=boxes, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "\n",
    "                max_logits = -999999\n",
    "                answer_page = None\n",
    "                document_outputs = None\n",
    "                for page_idx in range(len(document_encoding['input_ids'])):\n",
    "                    # input_ids = document_encoding[\"input_ids\"][page_idx].to(self.model.device)\n",
    "                    # attention_mask = document_encoding[\"attention_mask\"][page_idx].to(self.model.device)\n",
    "\n",
    "                    page_inputs = {k: v[page_idx].unsqueeze(dim=0) for k, v in document_encoding.items()}\n",
    "                    # Retrieval with logits is available only during inference and hence, the start and end indices are not used.\n",
    "                    # start_pos = torch.LongTensor(start_idxs).to(self.model.device) if start_idxs else None\n",
    "                    # end_pos = torch.LongTensor(end_idxs).to(self.model.device) if end_idxs else None\n",
    "\n",
    "                    page_outputs = self.model(**page_inputs)\n",
    "                    pred_answer, answer_conf = self.get_answer_from_model_output(page_inputs[\"input_ids\"].unsqueeze(dim=0), page_outputs)\n",
    "\n",
    "                    \"\"\"\n",
    "                    start_logits_cnf = [page_outputs.start_logits[batch_ix, max_start_logits_idx.item()].item() for batch_ix, max_start_logits_idx in enumerate(page_outputs.start_logits.argmax(-1))][0]\n",
    "                    end_logits_cnf = [page_outputs.end_logits[batch_ix, max_end_logits_idx.item()].item() for batch_ix, max_end_logits_idx in enumerate(page_outputs.end_logits.argmax(-1))][0]\n",
    "                    page_logits = np.mean([start_logits_cnf, end_logits_cnf])\n",
    "                    \"\"\"\n",
    "\n",
    "                    if answer_conf[0] > max_logits:\n",
    "                        answer_page = page_idx\n",
    "                        document_outputs = page_outputs\n",
    "                        max_logits = answer_conf[0]\n",
    "\n",
    "                outputs.append(None)  # outputs.append(document_outputs)  # During inference outputs are not used.\n",
    "                pred_answers.extend(self.get_answer_from_model_output([document_encoding[\"input_ids\"][answer_page]], document_outputs)[0] if return_pred_answer else None)\n",
    "                pred_answer_pages.append(answer_page)\n",
    "                answ_confidence.append(max_logits)\n",
    "\n",
    "        else:\n",
    "\n",
    "            boxes = [(bbox * 1000).astype(int) for bbox in batch['boxes']]  # Scale boxes 0->1 to 0-->1000.\n",
    "            encoding = self.processor(images, question, batch[\"words\"], boxes=boxes, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "\n",
    "            start_pos, end_pos = self.get_start_end_idx(encoding, context, answers)\n",
    "            outputs = self.model(**encoding, start_positions=start_pos, end_positions=end_pos)\n",
    "            pred_answers, answ_confidence = self.get_answer_from_model_output(encoding.input_ids, outputs) if return_pred_answer else None\n",
    "\n",
    "            if self.page_retrieval == 'oracle':\n",
    "                pred_answer_pages = batch['answer_page_idx']\n",
    "\n",
    "            elif self.page_retrieval == 'concat':\n",
    "                pred_answer_pages = [batch['context_page_corresp'][batch_idx][pred_start_idx] if len(batch['context_page_corresp'][batch_idx]) > pred_start_idx else -1 for batch_idx, pred_start_idx in enumerate(outputs.start_logits.argmax(-1).tolist())]\n",
    "\n",
    "            elif self.page_retrieval == 'none':\n",
    "                pred_answer_pages = None\n",
    "\n",
    "        return outputs, pred_answers, pred_answer_pages, answ_confidence\n",
    "\n",
    "    def get_concat_v_multi_resize(self, im_list, resample=Image.BICUBIC):\n",
    "        min_width = min(im.width for im in im_list)\n",
    "        im_list_resize = [im.resize((min_width, int(im.height * min_width / im.width)), resample=resample) for im in im_list]\n",
    "\n",
    "        # Fix equal height for all images (breaks the aspect ratio).\n",
    "        heights = [im.height for im in im_list]\n",
    "        im_list_resize = [im.resize((im.height, max(heights)), resample=resample) for im in im_list_resize]\n",
    "\n",
    "        total_height = sum(im.height for im in im_list_resize)\n",
    "        dst = Image.new('RGB', (min_width, total_height))\n",
    "        pos_y = 0\n",
    "        for im in im_list_resize:\n",
    "            dst.paste(im, (0, pos_y))\n",
    "            pos_y += im.height\n",
    "        return dst\n",
    "\n",
    "    def get_start_end_idx(self, encoding, context, answers):\n",
    "        pos_idx = []\n",
    "        for batch_idx in range(len(encoding.input_ids)):\n",
    "            answer_pos = []\n",
    "            for answer in answers[batch_idx]:\n",
    "                encoded_answer = [token for token in self.processor.tokenizer.encode([answer], boxes=[0, 0, 0, 0]) if token not in self.processor.tokenizer.all_special_ids]\n",
    "                answer_tokens_length = len(encoded_answer)\n",
    "\n",
    "                for token_pos in range(len(encoding.input_ids[batch_idx])):\n",
    "                    if encoding.input_ids[batch_idx][token_pos: token_pos+answer_tokens_length].tolist() == encoded_answer:\n",
    "                        answer_pos.append([token_pos, token_pos + answer_tokens_length-1])\n",
    "\n",
    "            if len(answer_pos) == 0:\n",
    "                pos_idx.append([self.ignore_index, self.ignore_index])\n",
    "\n",
    "            else:\n",
    "                answer_pos = random.choice(answer_pos)  # To add variability, pick a random correct span.\n",
    "                pos_idx.append(answer_pos)\n",
    "\n",
    "        start_idxs = torch.LongTensor([idx[0] for idx in pos_idx]).to(self.model.device)\n",
    "        end_idxs = torch.LongTensor([idx[1] for idx in pos_idx]).to(self.model.device)\n",
    "\n",
    "        return start_idxs, end_idxs\n",
    "\n",
    "    def get_answer_from_model_output(self, input_tokens, outputs):\n",
    "        start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "        end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "        answers = [self.processor.tokenizer.decode(input_tokens[batch_idx][start_idxs[batch_idx]: end_idxs[batch_idx]+1], skip_special_tokens=True).strip() for batch_idx in range(len(input_tokens))]\n",
    "        # answers_conf = ((outputs.start_logits.max(dim=1).values + outputs.end_logits.max(dim=1).values) / 2).tolist()\n",
    "\n",
    "        start_logits = outputs.start_logits.softmax(dim=1).detach().cpu()\n",
    "        end_logits = outputs.end_logits.softmax(dim=1).detach().cpu()\n",
    "        answ_confidence = []\n",
    "        for batch_idx in range(len(input_tokens)):\n",
    "            conf_mat = np.matmul(np.expand_dims(start_logits[batch_idx].unsqueeze(dim=0), -1),\n",
    "                                 np.expand_dims(end_logits[batch_idx].unsqueeze(dim=0), 1)).squeeze(axis=0)\n",
    "\n",
    "            answ_confidence.append(\n",
    "                conf_mat[start_idxs[batch_idx], end_idxs[batch_idx]].item()\n",
    "            )\n",
    "\n",
    "        aansw_confidence = model_utils.get_extractive_confidence(outputs)\n",
    "\n",
    "        return answers, answ_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA = LayoutLMv3ForQuestionAnswering()\n",
    "# processor = LayoutLMv3Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'batch_size':4,'model_weights':'microsoft/layoutlmv3-base'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['qa_outputs.dense.bias', 'qa_outputs.dense.weight', 'qa_outputs.out_proj.bias', 'qa_outputs.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "module = LayoutLMv3(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForQuestionAnswering(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3Processor:\n",
       "- image_processor: LayoutLMv3ImageProcessor {\n",
       "  \"apply_ocr\": false,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"LayoutLMv3ImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"ocr_lang\": null,\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"tesseract_config\": \"\"\n",
       "}\n",
       "\n",
       "- tokenizer: LayoutLMv3TokenizerFast(name_or_path='microsoft/layoutlmv3-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"LayoutLMv3Processor\"\n",
       "}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./data_select/task3_infographic/images/10022.jpeg')\n",
    "# module.processor(img, 'question', ['words'], boxes=[[1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor, AutoModel\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "# model = AutoModel.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "\n",
    "# dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n",
    "# example = dataset[0]\n",
    "# image = example[\"image\"]\n",
    "# words = example[\"tokens\"]\n",
    "# boxes = example[\"bboxes\"]\n",
    "\n",
    "# encoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model(**encoding)\n",
    "# last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

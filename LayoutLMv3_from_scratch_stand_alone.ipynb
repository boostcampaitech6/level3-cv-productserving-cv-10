{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, random#, json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn # as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import editdistance\n",
    "import socket, datetime#, getpass\n",
    "import wandb as wb\n",
    "import transformers\n",
    "from transformers import get_scheduler\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFDocVQA(Dataset):\n",
    "\n",
    "    def __init__(self, imbd_dir, images_dir, split, kwargs):\n",
    "        data = np.load(os.path.join(imbd_dir, \"infographics_imdb_{:s}.npy\".format(split)), allow_pickle=True)\n",
    "        self.imdb = data\n",
    "        self.images_dir = images_dir\n",
    "        self.use_images = kwargs.get('use_images', False)\n",
    "        self.get_raw_ocr_data = kwargs.get('get_raw_ocr_data', False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imdb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.imdb[idx]\n",
    "        question = record['question']\n",
    "        context = ' '.join([word.lower() for word in record['ocr_tokens']])\n",
    "        \n",
    "        if 'answers' in record :\n",
    "            answers = list(set(answer.lower() for answer in record['answers']))\n",
    "        else : \n",
    "            answers = ['0' * len(question)] \n",
    "        \n",
    "        if self.use_images:\n",
    "            image_name = os.path.join(self.images_dir, \"{:s}\".format(record['image_name']))\n",
    "            image = Image.open(image_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.get_raw_ocr_data:\n",
    "            words = [word.lower() for word in record['ocr_tokens']]\n",
    "            context = ' '.join([word.lower() for word in record['ocr_tokens']])\n",
    "            boxes = np.array([bbox for bbox in record['ocr_normalized_boxes']])\n",
    "\n",
    "        start_idxs, end_idxs = self._get_start_end_idx(context, answers)\n",
    "\n",
    "        sample_info = {'question_id': record['question_id'],\n",
    "                       'questions': question,\n",
    "                       'contexts': context,\n",
    "                       'answers': answers,\n",
    "                       'start_indxs': start_idxs,\n",
    "                       'end_indxs': end_idxs\n",
    "                       }        \n",
    "\n",
    "        if self.use_images:\n",
    "            sample_info['image_names'] = image_name\n",
    "            sample_info['images'] = image\n",
    "        \n",
    "\n",
    "        if self.get_raw_ocr_data:\n",
    "            sample_info['words'] = words\n",
    "            sample_info['boxes'] = boxes\n",
    "\n",
    "        else:  # Information for extractive models\n",
    "            sample_info['start_indxs'] = start_idxs\n",
    "            sample_info['end_indxs'] = end_idxs\n",
    "\n",
    "        return sample_info\n",
    "\n",
    "    def _get_start_end_idx(self, context, answers):\n",
    "\n",
    "        answer_positions = []\n",
    "        for answer in answers:\n",
    "            start_idx = context.find(answer)\n",
    "\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(answer)\n",
    "                answer_positions.append([start_idx, end_idx])\n",
    "\n",
    "        if len(answer_positions) > 0:\n",
    "            start_idx, end_idx = random.choice(answer_positions)  # If both answers are in the context. Choose one randomly.\n",
    "        else:\n",
    "            start_idx, end_idx = 0, 0  # If the indices are out of the sequence length they are ignored. Therefore, we set them as a very big number.\n",
    "\n",
    "        return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, split):\n",
    "    # Specify special params for data processing depending on the model used.\n",
    "    dataset_kwargs = {}\n",
    "    dataset_kwargs['get_raw_ocr_data'] = True\n",
    "    dataset_kwargs['use_images'] = True\n",
    "\n",
    "    # Build dataset\n",
    "    dataset = IFDocVQA(config['imdb_dir'], config['images_dir'], split, dataset_kwargs)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.log_folder = config['save_dir']\n",
    "        experiment_date = datetime.datetime.now().strftime('%Y.%m.%d_%H.%M.%S')\n",
    "        self.experiment_name = \"{:s}__{:}\".format(config['model_name'], experiment_date)\n",
    "\n",
    "        dataset = config['dataset_name']\n",
    "        visual_encoder = config.get('visual_module', {}).get('model', '-').upper()\n",
    "\n",
    "        tags = [config['model_name'], dataset]\n",
    "        config = {'Model': config['model_name'], 'Weights': config['model_weights'], 'Dataset': dataset,\n",
    "                  'Visual Encoder': visual_encoder,\n",
    "                  'Batch size': config['batch_size'], 'Max. Seq. Length': config.get('max_sequence_length', '-'),\n",
    "                  'lr': config['lr'], 'seed': config['seed']}\n",
    "\n",
    "        self.logger = wb.init(project=\"Hyunyoung in the house motherfuckers~\", name=self.experiment_name, dir=self.log_folder, tags=tags, config=config)\n",
    "        self._print_config(config)\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.len_dataset = 0\n",
    "\n",
    "    def _print_config(self, config):\n",
    "        print(\"{:s}: {:s} \\n{{\".format(config['Model'], config['Weights']))\n",
    "        for k, v in config.items():\n",
    "            if k != 'Model' and k != 'Weights':\n",
    "                print(\"\\t{:}: {:}\".format(k, v))\n",
    "        print(\"}\\n\")\n",
    "\n",
    "    def log_model_parameters(self, model):\n",
    "        total_params = sum(p.numel() for p in model.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "\n",
    "        self.logger.config.update({\n",
    "            'Model Params': int(total_params / 1e6),  # In millions\n",
    "            'Model Trainable Params': int(trainable_params / 1e6)  # In millions\n",
    "        })\n",
    "\n",
    "        print(\"Model parameters: {:d} - Trainable: {:d} ({:2.2f}%)\".format(\n",
    "            total_params, trainable_params, trainable_params / total_params * 100))\n",
    "\n",
    "    def log_val_metrics(self, accuracy, anls, update_best=False):\n",
    "        str_msg = \"Epoch {:d}: Accuracy {:2.2f}%    ANLS {:2.4f}\".format(self.current_epoch, accuracy*100, anls)\n",
    "        self.logger.log({\n",
    "            'Val/Epoch Accuracy': accuracy,\n",
    "            'Val/Epoch ANLS': anls,\n",
    "        }, step=self.current_epoch*self.len_dataset + self.len_dataset)\n",
    "\n",
    "        if update_best:\n",
    "            str_msg += \"\\tBest Accuracy!\"\n",
    "            self.logger.config.update({\n",
    "                \"Best Accuracy\": accuracy,\n",
    "                \"Best epoch\": self.current_epoch\n",
    "            }, allow_val_change=True)\n",
    "\n",
    "        print(str_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, case_sensitive=False):\n",
    "\n",
    "        self.case_sensitive = case_sensitive\n",
    "        self.get_edit_distance = editdistance.eval\n",
    "        self.anls_threshold = 0.5\n",
    "\n",
    "        self.total_accuracies = []\n",
    "        self.total_anls = []\n",
    "\n",
    "        self.best_accuracy = 0\n",
    "        self.best_anls = 0\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def get_metrics(self, gt_answers, preds, answer_types=None, update_global_metrics=True):\n",
    "        answer_types = answer_types if answer_types is not None else ['string' for batch_idx in range(len(gt_answers))]\n",
    "        batch_accuracy = []\n",
    "        batch_anls = []\n",
    "        for batch_idx in range(len(preds)):\n",
    "            gt = [self._preprocess_str(gt_elm) for gt_elm in gt_answers[batch_idx]]\n",
    "            pred = self._preprocess_str(preds[batch_idx])\n",
    "\n",
    "            batch_accuracy.append(self._calculate_accuracy(gt, pred, answer_types[batch_idx]))\n",
    "            batch_anls.append(self._calculate_anls(gt, pred, answer_types[batch_idx]))\n",
    "\n",
    "        # if accumulate_metrics:\n",
    "        #     self.total_accuracies.extend(batch_accuracy)\n",
    "        #     self.total_anls.extend(batch_anls)\n",
    "\n",
    "        return {'accuracy': batch_accuracy, 'anls': batch_anls}\n",
    "\n",
    "    def get_retrieval_metric(self, gt_answer_page, pred_answer_page):\n",
    "        retrieval_precision = [1 if gt == pred else 0 for gt, pred in zip(gt_answer_page, pred_answer_page)]\n",
    "        return retrieval_precision\n",
    "\n",
    "    def update_global_metrics(self, accuracy, anls, current_epoch):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_epoch = current_epoch\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _preprocess_str(self, string):\n",
    "        if not self.case_sensitive:\n",
    "            string = string.lower()\n",
    "\n",
    "        return string.strip()\n",
    "\n",
    "    def _calculate_accuracy(self, gt, pred, answer_type):\n",
    "\n",
    "        if answer_type == 'not-answerable':\n",
    "            return 1 if pred in ['', 'none', 'NA', None, []] else 0\n",
    "\n",
    "        if pred == 'none' and answer_type != 'not-answerable':\n",
    "            return 0\n",
    "\n",
    "        for gt_elm in gt:\n",
    "            if gt_elm == pred:\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _calculate_anls(self, gt, pred, answer_type):\n",
    "        if len(pred) == 0:\n",
    "            return 0\n",
    "\n",
    "        if answer_type == 'not-answerable':\n",
    "            return 1 if pred in ['', 'none', 'NA', None, []] else 0\n",
    "\n",
    "        if pred == 'none' and answer_type != 'not-answerable':\n",
    "            return 0\n",
    "\n",
    "        answers_similarity = [1 - self.get_edit_distance(gt_elm, pred) / max(len(gt_elm), len(pred)) for gt_elm in gt]\n",
    "        max_similarity = max(answers_similarity)\n",
    "\n",
    "        anls = max_similarity if max_similarity >= self.anls_threshold else 0\n",
    "        return anls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Info_docvqa_collate_fn(batch):\n",
    "    batch = {k: [dic[k] for dic in batch] for k in batch[0]}  # List of dictionaries to dict of lists.\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, length_train_loader, config):\n",
    "    optimizer_class = getattr(transformers, 'AdamW')\n",
    "    optimizer = optimizer_class(model.model.parameters(), lr=float(config['lr']))\n",
    "    num_training_steps = config['train_epochs'] * length_train_loader\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=config['warmup_iterations'], num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, evaluator, **kwargs):\n",
    "\n",
    "    return_scores_by_sample = kwargs.get('return_scores_by_sample', False)\n",
    "    return_answers = kwargs.get('return_answers', False)\n",
    "\n",
    "    if return_scores_by_sample:\n",
    "        scores_by_samples = {}\n",
    "        total_accuracies = []\n",
    "        total_anls = []\n",
    "\n",
    "    else:\n",
    "        total_accuracies = 0\n",
    "        total_anls = 0\n",
    "\n",
    "    all_pred_answers = []\n",
    "    model.model.eval()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "        bs = len(batch['question_id'])\n",
    "        with torch.no_grad():\n",
    "            # outputs, pred_answers, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "            _, pred_answers, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "            # print('real answer : ',batch['answers'])\n",
    "            # print('predicted answer : ',pred_answers)\n",
    "\n",
    "        metric = evaluator.get_metrics(batch['answers'], pred_answers, batch.get('answer_type', None))\n",
    "\n",
    "        if return_scores_by_sample:\n",
    "            for batch_idx in range(bs):\n",
    "                scores_by_samples[batch['question_id'][batch_idx]] = {\n",
    "                    'accuracy': metric['accuracy'][batch_idx],\n",
    "                    'anls': metric['anls'][batch_idx],\n",
    "                    'pred_answer': pred_answers[batch_idx],\n",
    "                    'pred_answer_conf': answer_conf[batch_idx],\n",
    "                    'image_names' : batch['image_names'][batch_idx], # 여기서 부터 추가한 부분\n",
    "                    'question' : batch['questions'][batch_idx], \n",
    "                }\n",
    "\n",
    "        if return_scores_by_sample:\n",
    "            total_accuracies.extend(metric['accuracy'])\n",
    "            total_anls.extend(metric['anls'])\n",
    "\n",
    "        else:\n",
    "            total_accuracies += sum(metric['accuracy'])\n",
    "            total_anls += sum(metric['anls'])\n",
    "\n",
    "        if return_answers:\n",
    "            all_pred_answers.extend(pred_answers)\n",
    "\n",
    "    if not return_scores_by_sample:\n",
    "        total_accuracies = total_accuracies/len(data_loader.dataset)\n",
    "        total_anls = total_anls/len(data_loader.dataset)\n",
    "        scores_by_samples = []\n",
    "    return total_accuracies, total_anls, all_pred_answers, scores_by_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yaml(path, data):\n",
    "    # print(data)\n",
    "    with open(path, 'w+') as f:\n",
    "        yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, update_best=False, **kwargs):\n",
    "    save_dir = os.path.join(kwargs['save_dir'], 'checkpoints', \"{:s}_{:s}\".format(kwargs['model_name'].lower(), kwargs['dataset_name'].lower()))\n",
    "\n",
    "    tokenizer = model.tokenizer if hasattr(model, 'tokenizer') else model.processor if hasattr(model, 'processor') else None\n",
    "\n",
    "    if update_best:\n",
    "        model.model.save_pretrained(os.path.join(save_dir, \"best.ckpt\"))\n",
    "        tokenizer.save_pretrained(os.path.join(save_dir, \"best.ckpt\"))\n",
    "        save_yaml(os.path.join(save_dir, \"best.ckpt\", \"experiment_config.yml\"), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, **kwargs):\n",
    "\n",
    "    epochs = kwargs['train_epochs']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    seed_everything(kwargs['seed'])\n",
    "\n",
    "    evaluator = Evaluator(case_sensitive=False)\n",
    "    logger = Logger(config=kwargs)\n",
    "    logger.log_model_parameters(model)\n",
    "\n",
    "    train_dataset = build_dataset(config, 'train')\n",
    "    val_dataset   = build_dataset(config, 'val')\n",
    "    print('done')\n",
    "    # g = torch.Generator()\n",
    "    # g.manual_seed(kwargs['seed'])\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=Info_docvqa_collate_fn)\n",
    "    val_data_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=Info_docvqa_collate_fn)\n",
    "\n",
    "    logger.len_dataset = len(train_data_loader)\n",
    "    optimizer, lr_scheduler = build_optimizer(model, length_train_loader=len(train_data_loader), config=kwargs)\n",
    "\n",
    "    if kwargs.get('eval_start', False):\n",
    "        logger.current_epoch = -1\n",
    "        accuracy, anls, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)\n",
    "        is_updated = evaluator.update_global_metrics(accuracy, anls, -1)\n",
    "        logger.log_val_metrics(accuracy, anls, update_best=is_updated)\n",
    "\n",
    "    for epoch_ix in range(epochs):\n",
    "        logger.current_epoch = epoch_ix\n",
    "        train_epoch(train_data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs)\n",
    "        accuracy, anls, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)\n",
    "        is_updated = evaluator.update_global_metrics(accuracy, anls, epoch_ix)\n",
    "        logger.log_val_metrics(accuracy, anls, update_best=is_updated)\n",
    "        save_model(model, epoch_ix, update_best=is_updated, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs):\n",
    "    model.model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "        gt_answers = batch['answers']\n",
    "        outputs, pred_answers, _ = model.forward(batch, return_pred_answer=True)\n",
    "\n",
    "        loss = outputs.loss\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        metric = evaluator.get_metrics(gt_answers, pred_answers)\n",
    "\n",
    "        batch_acc = np.mean(metric['accuracy'])\n",
    "        batch_anls = np.mean(metric['anls'])\n",
    "\n",
    "        log_dict = {\n",
    "            'Train/Batch loss': outputs.loss.item(),\n",
    "            'Train/Batch Accuracy': batch_acc,         \n",
    "            'Train/Batch ANLS': batch_anls,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        logger.logger.log(log_dict, step=logger.current_epoch * logger.len_dataset + batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractive_confidence(outputs):\n",
    "    bs = len(outputs['start_logits'])\n",
    "    start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "    end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "    answ_confidence = []\n",
    "    for batch_idx in range(bs):\n",
    "        conf_mat = np.matmul(np.expand_dims(outputs.start_logits.softmax(dim=1)[batch_idx].unsqueeze(dim=0).detach().cpu(), -1),\n",
    "                             np.expand_dims(outputs.end_logits.softmax(dim=1)[batch_idx].unsqueeze(dim=0).detach().cpu(), 1)).squeeze(axis=0)\n",
    "\n",
    "        answ_confidence.append(\n",
    "            conf_mat[start_idxs[batch_idx], end_idxs[batch_idx]].item()\n",
    "        )\n",
    "\n",
    "    return answ_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutLMv3_hy:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.apply_ocr = config['apply_ocr']\n",
    "        self.processor = LayoutLMv3Processor.from_pretrained(config['model_weights'], apply_ocr=config['apply_ocr'])  # Check that this do not fuck up the code.\n",
    "        self.model = LayoutLMv3ForQuestionAnswering.from_pretrained(config['model_weights'])\n",
    "        self.ignore_index = 9999  # 0\n",
    "\n",
    "    # def parallelize(self):\n",
    "    #     self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, batch, return_pred_answer=False):\n",
    "\n",
    "        # bs = len(batch['question_id'])\n",
    "        question = batch['questions']\n",
    "        context = batch['contexts']\n",
    "        answers = batch['answers']\n",
    "        images = batch['images']\n",
    "\n",
    "        boxes = [(bbox * 1000).astype(int) for bbox in batch['boxes']]  # Scale boxes 0->1 to 0-->1000.\n",
    "        \n",
    "        if self.apply_ocr:\n",
    "            encoding = self.processor(images, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "        else:\n",
    "            encoding = self.processor(images, question, batch[\"words\"], boxes=boxes, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "\n",
    "        start_pos, end_pos = self.get_start_end_idx(encoding, context, answers)\n",
    "        outputs = self.model(**encoding, start_positions=start_pos, end_positions=end_pos)\n",
    "        pred_answers, answ_confidence = self.get_answer_from_model_output(encoding.input_ids, outputs) if return_pred_answer else None\n",
    "\n",
    "        return outputs, pred_answers, answ_confidence\n",
    "\n",
    "    def get_concat_v_multi_resize(self, im_list, resample=Image.BICUBIC):\n",
    "        min_width = min(im.width for im in im_list)\n",
    "        im_list_resize = [im.resize((min_width, int(im.height * min_width / im.width)), resample=resample) for im in im_list]\n",
    "\n",
    "        # Fix equal height for all images (breaks the aspect ratio).\n",
    "        heights = [im.height for im in im_list]\n",
    "        im_list_resize = [im.resize((im.height, max(heights)), resample=resample) for im in im_list_resize]\n",
    "\n",
    "        total_height = sum(im.height for im in im_list_resize)\n",
    "        dst = Image.new('RGB', (min_width, total_height))\n",
    "        pos_y = 0\n",
    "        for im in im_list_resize:\n",
    "            dst.paste(im, (0, pos_y))\n",
    "            pos_y += im.height\n",
    "        return dst\n",
    "\n",
    "    def get_start_end_idx(self, encoding, context, answers):\n",
    "        pos_idx = []\n",
    "        for batch_idx in range(len(encoding.input_ids)):\n",
    "            answer_pos = []\n",
    "            for answer in answers[batch_idx]:\n",
    "                encoded_answer = [token for token in self.processor.tokenizer.encode([answer], boxes=[0, 0, 0, 0]) if token not in self.processor.tokenizer.all_special_ids]\n",
    "                answer_tokens_length = len(encoded_answer)\n",
    "\n",
    "                for token_pos in range(len(encoding.input_ids[batch_idx])):\n",
    "                    if encoding.input_ids[batch_idx][token_pos: token_pos+answer_tokens_length].tolist() == encoded_answer:\n",
    "                        answer_pos.append([token_pos, token_pos + answer_tokens_length-1])\n",
    "\n",
    "            if len(answer_pos) == 0:\n",
    "                pos_idx.append([self.ignore_index, self.ignore_index])\n",
    "\n",
    "            else:\n",
    "                answer_pos = random.choice(answer_pos)  # To add variability, pick a random correct span.\n",
    "                pos_idx.append(answer_pos)\n",
    "\n",
    "        start_idxs = torch.LongTensor([idx[0] for idx in pos_idx]).to(self.model.device)\n",
    "        end_idxs = torch.LongTensor([idx[1] for idx in pos_idx]).to(self.model.device)\n",
    "\n",
    "        return start_idxs, end_idxs\n",
    "\n",
    "    def get_answer_from_model_output(self, input_tokens, outputs):\n",
    "        predicted_start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "        predicted_end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "        predicted_answers = [self.processor.tokenizer.decode(input_tokens[batch_idx][predicted_start_idxs[batch_idx]: predicted_end_idxs[batch_idx]+1], skip_special_tokens=True).strip() for batch_idx in range(len(input_tokens))]\n",
    "        # answers_conf = ((outputs.start_logits.max(dim=1).values + outputs.end_logits.max(dim=1).values) / 2).tolist()\n",
    "\n",
    "        start_logits = outputs.start_logits.softmax(dim=1).detach().cpu()\n",
    "        end_logits = outputs.end_logits.softmax(dim=1).detach().cpu()\n",
    "        answ_confidence = []\n",
    "        for batch_idx in range(len(input_tokens)):\n",
    "            conf_mat = np.matmul(np.expand_dims(start_logits[batch_idx].unsqueeze(dim=0), -1),\n",
    "                                 np.expand_dims(end_logits[batch_idx].unsqueeze(dim=0), 1)).squeeze(axis=0)\n",
    "\n",
    "            answ_confidence.append(\n",
    "                conf_mat[predicted_start_idxs[batch_idx], predicted_end_idxs[batch_idx]].item()\n",
    "            )\n",
    "\n",
    "        answ_confidence = get_extractive_confidence(outputs)\n",
    "\n",
    "        return predicted_answers, answ_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    # \"model\": \"hy\",\n",
    "    \"dataset\": \"infographics\",\n",
    "    \"eval_start\": True,\n",
    "    \"no_eval_start\": False,\n",
    "    \"page_retrieval\": None,\n",
    "    \"batch_size\": None,\n",
    "    \"max_sequence_length\": None,\n",
    "    \"seed\": 42,\n",
    "    \"save_dir\": \"saving_dir/\",\n",
    "    \"apply_ocr\": False,\n",
    "    \"data_parallel\": False,\n",
    "    \"no_data_parallel\": False,\n",
    "    \"model_name\": \"hy\",\n",
    "    \"model_weights\": \"microsoft/layoutlmv3-base\",\n",
    "    \"device\": \"cuda\",\n",
    "    # \"training_parameters\": {\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"train_epochs\": 40,\n",
    "    \"warmup_iterations\": 50,\n",
    "    # },\n",
    "    \"dataset_name\": \"infographicVQA\",\n",
    "    \"imdb_dir\": \"./hy_info/task3/imdb\",\n",
    "    \"images_dir\": \"./hy_info/task3/images\",\n",
    "    # \"imdb_dir\": \"./hy_info/Task3_test/imdb\",\n",
    "    # \"images_dir\": \"./hy_info/Task3_test/images\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['qa_outputs.dense.bias', 'qa_outputs.dense.weight', 'qa_outputs.out_proj.bias', 'qa_outputs.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbigchoi3449\u001b[0m (\u001b[33mlevel2-cv-10-detection\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /home/chy/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 854 getcaller\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m.\u001b[39mparallelize()\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m train(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m seed_everything(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(case_sensitive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(config\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_model_parameters(model)\n\u001b[1;32m     11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m build_dataset(config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mLogger.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     11\u001b[0m tags \u001b[38;5;241m=\u001b[39m [config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset]\n\u001b[1;32m     12\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset,\n\u001b[1;32m     13\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisual Encoder\u001b[39m\u001b[38;5;124m'\u001b[39m: visual_encoder,\n\u001b[1;32m     14\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch size\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax. Seq. Length\u001b[39m\u001b[38;5;124m'\u001b[39m: config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_sequence_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     15\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m wb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyunyoung in the house motherfuckers~\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_name, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_folder, tags\u001b[38;5;241m=\u001b[39mtags, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_config(config)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1199\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[1;32m   1198\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m-> 1199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1201\u001b[0m     error_seen \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1176\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1174\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     run \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m   1177\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:817\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m run_start_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run_start(run\u001b[38;5;241m.\u001b[39m_run_obj)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# TODO: add progress to let user know we are doing something\u001b[39;00m\n\u001b[0;32m--> 817\u001b[0m run_start_result \u001b[38;5;241m=\u001b[39m run_start_handle\u001b[38;5;241m.\u001b[39mwait(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_start_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     run_start_handle\u001b[38;5;241m.\u001b[39mabandon()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot\u001b[38;5;241m.\u001b[39m_get_and_clear(timeout\u001b[38;5;241m=\u001b[39mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout\u001b[38;5;241m=\u001b[39mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = args_dict\n",
    "# config.pop('model')\n",
    "model_name = config['model_name'].lower()\n",
    "if 'save_dir' in config:\n",
    "    if not config['save_dir'].endswith('/'):\n",
    "        config['save_dir'] = config['save_dir'] + '/'\n",
    "\n",
    "    if not os.path.exists(config['save_dir']):\n",
    "        os.makedirs(config['save_dir'])\n",
    "\n",
    "# if 'seed' not in config:\n",
    "#     print(\"Seed not specified. Setting default seed to '{:d}'\".format(42))\n",
    "#     config['seed'] = 42\n",
    "\n",
    "model = LayoutLMv3_hy(config)\n",
    "\n",
    "if config['device'] == 'cuda' and config['data_parallel'] and torch.cuda.device_count() > 1:\n",
    "    model.parallelize()\n",
    "\n",
    "model.model.to(config['device'])\n",
    "\n",
    "train(model, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정리\n",
    "#1 이미지 받기\n",
    "image = Image.open('~~~~')\n",
    "#2 퀘스쳔 받기\n",
    "question = 'what is this?'\n",
    "\n",
    "#make imdb of image question pair\n",
    "\n",
    "#dataloader\n",
    "test_data_loader   = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=Info_docvqa_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연산\n",
    "\n",
    "# gt_answers = batch['answers']\n",
    "# outputs, pred_answers, _ = model.forward(batch, return_pred_answer=True)\n",
    "evaluator = Evaluator(case_sensitive=False)\n",
    "accuracy, anls, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

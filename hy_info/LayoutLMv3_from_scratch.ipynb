{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn # as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os, yaml#, json\n",
    "import editdistance\n",
    "import socket, datetime#, getpass\n",
    "import wandb as wb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import transformers\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\n",
    "from PIL import Image\n",
    "# import models._model_utils as model_utils\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs):\n",
    "    model.model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "        gt_answers = batch['answers']\n",
    "        # outputs, pred_answers, pred_answer_page, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "        # outputs, pred_answers, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "        # print(len(model.forward(batch, return_pred_answer=True)))\n",
    "        outputs, pred_answers, _ = model.forward(batch, return_pred_answer=True)\n",
    "\n",
    "        # loss = outputs.loss + outputs.ret_loss if hasattr(outputs, 'ret_loss') else outputs.loss\n",
    "        loss = outputs.loss\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        metric = evaluator.get_metrics(gt_answers, pred_answers)\n",
    "\n",
    "        batch_acc = np.mean(metric['accuracy'])\n",
    "        batch_anls = np.mean(metric['anls'])\n",
    "\n",
    "        log_dict = {\n",
    "            'Train/Batch loss': outputs.loss.item(),\n",
    "            'Train/Batch Accuracy': batch_acc,         \n",
    "            'Train/Batch ANLS': batch_anls,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        # if hasattr(outputs, 'ret_loss'):\n",
    "        #     log_dict['Train/Batch retrieval loss'] = outputs.ret_loss.item()\n",
    "\n",
    "        # if 'answer_page_idx' in batch and None not in batch['answer_page_idx']:\n",
    "        #     ret_metric = evaluator.get_retrieval_metric(batch.get('answer_page_idx', None), pred_answer_page)\n",
    "        #     batch_ret_prec = np.mean(ret_metric)\n",
    "        #     log_dict['Train/Batch Ret. Prec.'] = batch_ret_prec\n",
    "\n",
    "        logger.logger.log(log_dict, step=logger.current_epoch * logger.len_dataset + batch_idx)\n",
    "\n",
    "    # return total_accuracies, total_anls, answers\n",
    "\n",
    "\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2 ** 32\n",
    "#     np.random.seed(worker_seed)\n",
    "#     np.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFDocVQA(Dataset):\n",
    "\n",
    "    def __init__(self, imbd_dir, images_dir, split, kwargs):\n",
    "        data = np.load(os.path.join(imbd_dir, \"infographics_imdb_{:s}.npy\".format(split)), allow_pickle=True)\n",
    "        # self.header = data[0]\n",
    "        # self.imdb = data[1:]\n",
    "        self.imdb = data\n",
    "\n",
    "        self.max_answers = 2\n",
    "        self.images_dir = images_dir\n",
    "\n",
    "        # self.hierarchical_method = kwargs.get('hierarchical_method', False)\n",
    "        self.use_images = kwargs.get('use_images', False)\n",
    "        self.get_raw_ocr_data = kwargs.get('get_raw_ocr_data', False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imdb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.imdb[idx]\n",
    "        question = record['question']\n",
    "        context = ' '.join([word.lower() for word in record['ocr_tokens']])\n",
    "        # context_page_corresp = [0 for ix in range(len(context))]  # This is used to predict the answer page in MP-DocVQA. To keep it simple, use a mock list with corresponding page to 0.\n",
    "        \n",
    "        #나중에 if문 걸어서 train/inference 따로 \n",
    "        if 'answers' in record :\n",
    "            answers = list(set(answer.lower() for answer in record['answers']))\n",
    "        else : \n",
    "            answers = ['0' * len(question)] \n",
    "        # answers = list(set(answer.lower() for answer in record.get('answers', [])))\n",
    "        \n",
    "        if self.use_images:\n",
    "            # image_name = os.path.join(self.images_dir, \"{:s}.png\".format(record['image_name']))\n",
    "            image_name = os.path.join(self.images_dir, \"{:s}\".format(record['image_name']))\n",
    "            image = Image.open(image_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.get_raw_ocr_data:\n",
    "            words = [word.lower() for word in record['ocr_tokens']]\n",
    "            context = ' '.join([word.lower() for word in record['ocr_tokens']])\n",
    "            boxes = np.array([bbox for bbox in record['ocr_normalized_boxes']])\n",
    "        # else:\n",
    "        #     words = \n",
    "        #     context = procsec\n",
    "        #     boxes = \n",
    "            \n",
    "        # if self.hierarchical_method:\n",
    "        #     words = [words]\n",
    "        #     boxes = [boxes]\n",
    "        #     image_name = [image_name]\n",
    "        #     image = [image]\n",
    "\n",
    "        start_idxs, end_idxs = self._get_start_end_idx(context, answers)\n",
    "\n",
    "        sample_info = {'question_id': record['question_id'],\n",
    "                       'questions': question,\n",
    "                       'contexts': context,\n",
    "                       'answers': answers,\n",
    "                       'start_indxs': start_idxs,\n",
    "                       'end_indxs': end_idxs\n",
    "                       }        \n",
    "\n",
    "        if self.use_images:\n",
    "            sample_info['image_names'] = image_name\n",
    "            sample_info['images'] = image\n",
    "        \n",
    "\n",
    "        if self.get_raw_ocr_data:\n",
    "            sample_info['words'] = words\n",
    "            sample_info['boxes'] = boxes\n",
    "            # sample_info['num_pages'] = 1\n",
    "            # sample_info['answer_page_idx'] = 0\n",
    "\n",
    "        else:  # Information for extractive models\n",
    "            # sample_info['context_page_corresp'] = context_page_corresp\n",
    "            sample_info['start_indxs'] = start_idxs\n",
    "            sample_info['end_indxs'] = end_idxs\n",
    "\n",
    "        return sample_info\n",
    "\n",
    "    def _get_start_end_idx(self, context, answers):\n",
    "\n",
    "        answer_positions = []\n",
    "        for answer in answers:\n",
    "            start_idx = context.find(answer)\n",
    "\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(answer)\n",
    "                answer_positions.append([start_idx, end_idx])\n",
    "\n",
    "        if len(answer_positions) > 0:\n",
    "            start_idx, end_idx = random.choice(answer_positions)  # If both answers are in the context. Choose one randomly.\n",
    "        else:\n",
    "            start_idx, end_idx = 0, 0  # If the indices are out of the sequence length they are ignored. Therefore, we set them as a very big number.\n",
    "\n",
    "        return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, split):\n",
    "\n",
    "    # Specify special params for data processing depending on the model used.\n",
    "    dataset_kwargs = {}\n",
    "    dataset_kwargs['get_raw_ocr_data'] = True\n",
    "    dataset_kwargs['use_images'] = True\n",
    "\n",
    "    # Build dataset\n",
    "    # from datasets.IF_DocVQA import IFDocVQA\n",
    "    dataset = IFDocVQA(config['imdb_dir'], config['images_dir'], split, dataset_kwargs)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.log_folder = config['save_dir']\n",
    "\n",
    "        experiment_date = datetime.datetime.now().strftime('%Y.%m.%d_%H.%M.%S')\n",
    "        self.experiment_name = \"{:s}__{:}\".format(config['model_name'], experiment_date)\n",
    "\n",
    "        machine_dict = {'cvc117': 'Local', 'cudahpc16': 'DAG', 'cudahpc25': 'DAG-A40'}\n",
    "        machine = machine_dict.get(socket.gethostname(), socket.gethostname())\n",
    "\n",
    "        dataset = config['dataset_name']\n",
    "        # page_retrieval = config.get('page_retrieval', '-').capitalize()\n",
    "        visual_encoder = config.get('visual_module', {}).get('model', '-').upper()\n",
    "\n",
    "        document_pages = config.get('max_pages', None)\n",
    "        page_tokens = config.get('page_tokens', None)\n",
    "        tags = [config['model_name'], dataset, machine]\n",
    "        config = {'Model': config['model_name'], 'Weights': config['model_weights'], 'Dataset': dataset,\n",
    "                  'Visual Encoder': visual_encoder,\n",
    "                  'Batch size': config['batch_size'], 'Max. Seq. Length': config.get('max_sequence_length', '-'),\n",
    "                  'lr': config['lr'], 'seed': config['seed']}\n",
    "\n",
    "        if document_pages:\n",
    "            config['Max Pages'] = document_pages\n",
    "\n",
    "        if page_tokens:\n",
    "            config['PAGE tokens'] = page_tokens\n",
    "\n",
    "        # self.logger = wb.init(project=\"MP-DocVQA\", name=self.experiment_name, dir=self.log_folder, tags=tags, config=config)\n",
    "        self.logger = wb.init(project=\"Hyunyoung in the house motherfuckers~\", name=self.experiment_name, dir=self.log_folder, tags=tags, config=config)\n",
    "        self._print_config(config)\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.len_dataset = 0\n",
    "\n",
    "    def _print_config(self, config):\n",
    "        print(\"{:s}: {:s} \\n{{\".format(config['Model'], config['Weights']))\n",
    "        for k, v in config.items():\n",
    "            if k != 'Model' and k != 'Weights':\n",
    "                print(\"\\t{:}: {:}\".format(k, v))\n",
    "        print(\"}\\n\")\n",
    "\n",
    "    def log_model_parameters(self, model):\n",
    "        total_params = sum(p.numel() for p in model.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "\n",
    "        self.logger.config.update({\n",
    "            'Model Params': int(total_params / 1e6),  # In millions\n",
    "            'Model Trainable Params': int(trainable_params / 1e6)  # In millions\n",
    "        })\n",
    "\n",
    "        print(\"Model parameters: {:d} - Trainable: {:d} ({:2.2f}%)\".format(\n",
    "            total_params, trainable_params, trainable_params / total_params * 100))\n",
    "\n",
    "    def log_val_metrics(self, accuracy, anls, ret_prec, update_best=False):\n",
    "\n",
    "        str_msg = \"Epoch {:d}: Accuracy {:2.2f}     ANLS {:2.4f}    Retrieval precision: {:2.2f}%\".format(self.current_epoch, accuracy*100, anls, ret_prec*100)\n",
    "        self.logger.log({\n",
    "            'Val/Epoch Accuracy': accuracy,\n",
    "            'Val/Epoch ANLS': anls,\n",
    "            'Val/Epoch Ret. Prec': ret_prec,\n",
    "        }, step=self.current_epoch*self.len_dataset + self.len_dataset)\n",
    "\n",
    "        if update_best:\n",
    "            str_msg += \"\\tBest Accuracy!\"\n",
    "            self.logger.config.update({\n",
    "                \"Best Accuracy\": accuracy,\n",
    "                \"Best epoch\": self.current_epoch\n",
    "            }, allow_val_change=True)\n",
    "\n",
    "        print(str_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, case_sensitive=False):\n",
    "\n",
    "        self.case_sensitive = case_sensitive\n",
    "        self.get_edit_distance = editdistance.eval\n",
    "        self.anls_threshold = 0.5\n",
    "\n",
    "        self.total_accuracies = []\n",
    "        self.total_anls = []\n",
    "\n",
    "        self.best_accuracy = 0\n",
    "        # self.best_anls = 0\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def get_metrics(self, gt_answers, preds, answer_types=None, update_global_metrics=True):\n",
    "        answer_types = answer_types if answer_types is not None else ['string' for batch_idx in range(len(gt_answers))]\n",
    "        batch_accuracy = []\n",
    "        batch_anls = []\n",
    "        for batch_idx in range(len(preds)):\n",
    "            gt = [self._preprocess_str(gt_elm) for gt_elm in gt_answers[batch_idx]]\n",
    "            pred = self._preprocess_str(preds[batch_idx])\n",
    "\n",
    "            batch_accuracy.append(self._calculate_accuracy(gt, pred, answer_types[batch_idx]))\n",
    "            batch_anls.append(self._calculate_anls(gt, pred, answer_types[batch_idx]))\n",
    "\n",
    "        # if accumulate_metrics:\n",
    "        #     self.total_accuracies.extend(batch_accuracy)\n",
    "        #     self.total_anls.extend(batch_anls)\n",
    "\n",
    "        return {'accuracy': batch_accuracy, 'anls': batch_anls}\n",
    "\n",
    "    def get_retrieval_metric(self, gt_answer_page, pred_answer_page):\n",
    "        retrieval_precision = [1 if gt == pred else 0 for gt, pred in zip(gt_answer_page, pred_answer_page)]\n",
    "        return retrieval_precision\n",
    "\n",
    "    def update_global_metrics(self, accuracy, anls, current_epoch):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_epoch = current_epoch\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _preprocess_str(self, string):\n",
    "        if not self.case_sensitive:\n",
    "            string = string.lower()\n",
    "\n",
    "        return string.strip()\n",
    "\n",
    "    def _calculate_accuracy(self, gt, pred, answer_type):\n",
    "\n",
    "        if answer_type == 'not-answerable':\n",
    "            return 1 if pred in ['', 'none', 'NA', None, []] else 0\n",
    "\n",
    "        if pred == 'none' and answer_type != 'not-answerable':\n",
    "            return 0\n",
    "\n",
    "        for gt_elm in gt:\n",
    "            if gt_elm == pred:\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _calculate_anls(self, gt, pred, answer_type):\n",
    "        if len(pred) == 0:\n",
    "            return 0\n",
    "\n",
    "        if answer_type == 'not-answerable':\n",
    "            return 1 if pred in ['', 'none', 'NA', None, []] else 0\n",
    "\n",
    "        if pred == 'none' and answer_type != 'not-answerable':\n",
    "            return 0\n",
    "\n",
    "        answers_similarity = [1 - self.get_edit_distance(gt_elm, pred) / max(len(gt_elm), len(pred)) for gt_elm in gt]\n",
    "        max_similarity = max(answers_similarity)\n",
    "\n",
    "        anls = max_similarity if max_similarity >= self.anls_threshold else 0\n",
    "        return anls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Info_docvqa_collate_fn(batch):\n",
    "    batch = {k: [dic[k] for dic in batch] for k in batch[0]}  # List of dictionaries to dict of lists.\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, length_train_loader, config):\n",
    "    optimizer_class = getattr(transformers, 'AdamW')\n",
    "    optimizer = optimizer_class(model.model.parameters(), lr=float(config['lr']))\n",
    "    num_training_steps = config['train_epochs'] * length_train_loader\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=config['warmup_iterations'], num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, evaluator, **kwargs):\n",
    "\n",
    "    return_scores_by_sample = kwargs.get('return_scores_by_sample', False)\n",
    "    return_answers = kwargs.get('return_answers', False)\n",
    "\n",
    "    if return_scores_by_sample:\n",
    "        scores_by_samples = {}\n",
    "        total_accuracies = []\n",
    "        total_anls = []\n",
    "        total_ret_prec = []\n",
    "\n",
    "    else:\n",
    "        total_accuracies = 0\n",
    "        total_anls = 0\n",
    "        total_ret_prec = 0\n",
    "\n",
    "    all_pred_answers = []\n",
    "    model.model.eval()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "        bs = len(batch['question_id'])\n",
    "        with torch.no_grad():\n",
    "            # outputs, pred_answers, pred_answer_page, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "            outputs, pred_answers, answer_conf = model.forward(batch, return_pred_answer=True)\n",
    "            # print(pred_answers)\n",
    "\n",
    "        metric = evaluator.get_metrics(batch['answers'], pred_answers, batch.get('answer_type', None))\n",
    "\n",
    "        # if 'answer_page_idx' in batch and pred_answer_page is not None:\n",
    "        #     ret_metric = evaluator.get_retrieval_metric(batch['answer_page_idx'], pred_answer_page)\n",
    "        # else:\n",
    "        #     ret_metric = [0 for _ in range(bs)]\n",
    "\n",
    "        if return_scores_by_sample:\n",
    "            for batch_idx in range(bs):\n",
    "                #TODO : 여기다가 만약 inference 단계이면 answer 필요한 부분 다 빼기\n",
    "                scores_by_samples[batch['question_id'][batch_idx]] = {\n",
    "                    'accuracy': metric['accuracy'][batch_idx],\n",
    "                    'anls': metric['anls'][batch_idx],\n",
    "                    # 'ret_prec': ret_metric[batch_idx],\n",
    "                    'pred_answer': pred_answers[batch_idx],\n",
    "                    'pred_answer_conf': answer_conf[batch_idx],\n",
    "                    # 'pred_answer_page': pred_answer_page[batch_idx] if pred_answer_page is not None else None,\n",
    "                    'image_names' : batch['image_names'][batch_idx], # 여기서 부터 추가한 부분\n",
    "                    'question' : batch['questions'][batch_idx], \n",
    "\n",
    "                    \n",
    "                }\n",
    "\n",
    "        if return_scores_by_sample:\n",
    "            total_accuracies.extend(metric['accuracy'])\n",
    "            total_anls.extend(metric['anls'])\n",
    "            # total_ret_prec.extend(ret_metric)\n",
    "\n",
    "        else:\n",
    "            total_accuracies += sum(metric['accuracy'])\n",
    "            total_anls += sum(metric['anls'])\n",
    "            # total_ret_prec += sum(ret_metric)\n",
    "\n",
    "        if return_answers:\n",
    "            all_pred_answers.extend(pred_answers)\n",
    "\n",
    "    if not return_scores_by_sample:\n",
    "        total_accuracies = total_accuracies/len(data_loader.dataset)\n",
    "        total_anls = total_anls/len(data_loader.dataset)\n",
    "        total_ret_prec = total_ret_prec/len(data_loader.dataset)\n",
    "        scores_by_samples = []\n",
    "\n",
    "    return total_accuracies, total_anls, total_ret_prec, all_pred_answers, scores_by_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yaml(path, data):\n",
    "    # print(data)\n",
    "    with open(path, 'w+') as f:\n",
    "        yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, update_best=False, **kwargs):\n",
    "    save_dir = os.path.join(kwargs['save_dir'], 'checkpoints', \"{:s}_{:s}\".format(kwargs['model_name'].lower(), kwargs['dataset_name'].lower()))\n",
    "    # model.model.save_pretrained(os.path.join(save_dir, \"model__{:d}.ckpt\".format(epoch)))\n",
    "\n",
    "    tokenizer = model.tokenizer if hasattr(model, 'tokenizer') else model.processor if hasattr(model, 'processor') else None\n",
    "    # if tokenizer is not None:\n",
    "    #     tokenizer.save_pretrained(os.path.join(save_dir, \"model__{:d}.ckpt\".format(epoch)))\n",
    "\n",
    "    # if hasattr(model.model, 'visual_embeddings'):\n",
    "    #     model.model.visual_embeddings.feature_extractor.save_pretrained(os.path.join(save_dir, \"model__{:d}.ckpt\".format(epoch)))\n",
    "\n",
    "    # save_yaml(os.path.join(save_dir, \"model__{:d}.ckpt\".format(epoch), \"experiment_config.yml\"), kwargs)\n",
    "\n",
    "    if update_best:\n",
    "        model.model.save_pretrained(os.path.join(save_dir, \"best.ckpt\"))\n",
    "        tokenizer.save_pretrained(os.path.join(save_dir, \"best.ckpt\"))\n",
    "        save_yaml(os.path.join(save_dir, \"best.ckpt\", \"experiment_config.yml\"), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, **kwargs):\n",
    "\n",
    "    epochs = kwargs['train_epochs']\n",
    "    # device = kwargs['device']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    seed_everything(kwargs['seed'])\n",
    "\n",
    "    evaluator = Evaluator(case_sensitive=False)\n",
    "    logger = Logger(config=kwargs)\n",
    "    logger.log_model_parameters(model)\n",
    "\n",
    "    train_dataset = build_dataset(config, 'train')\n",
    "    val_dataset   = build_dataset(config, 'val')\n",
    "    print('done')\n",
    "    # g = torch.Generator()\n",
    "    # g.manual_seed(kwargs['seed'])\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=Info_docvqa_collate_fn)\n",
    "    val_data_loader   = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=Info_docvqa_collate_fn)\n",
    "    # train_data_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=singledocvqa_collate_fn, worker_init_fn=seed_worker, generator=g)\n",
    "    # val_data_loader   = DataLoader(val_dataset, batch_size=config['batch_size'],  shuffle=False, collate_fn=singledocvqa_collate_fn, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    logger.len_dataset = len(train_data_loader)\n",
    "    optimizer, lr_scheduler = build_optimizer(model, length_train_loader=len(train_data_loader), config=kwargs)\n",
    "\n",
    "    if kwargs.get('eval_start', False):\n",
    "        logger.current_epoch = -1\n",
    "        accuracy, anls, ret_prec, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)\n",
    "        is_updated = evaluator.update_global_metrics(accuracy, anls, -1)\n",
    "        logger.log_val_metrics(accuracy, anls, ret_prec, update_best=is_updated)\n",
    "\n",
    "    for epoch_ix in range(epochs):\n",
    "        logger.current_epoch = epoch_ix\n",
    "        train_epoch(train_data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs)\n",
    "        accuracy, anls, ret_prec, _, _ = evaluate(val_data_loader, model, evaluator, return_scores_by_sample=False, return_pred_answers=False, **kwargs)\n",
    "        is_updated = evaluator.update_global_metrics(accuracy, anls, epoch_ix)\n",
    "        logger.log_val_metrics(accuracy, anls, ret_prec, update_best=is_updated)\n",
    "        save_model(model, epoch_ix, update_best=is_updated, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractive_confidence(outputs):\n",
    "    bs = len(outputs['start_logits'])\n",
    "    start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "    end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "    answ_confidence = []\n",
    "    for batch_idx in range(bs):\n",
    "        conf_mat = np.matmul(np.expand_dims(outputs.start_logits.softmax(dim=1)[batch_idx].unsqueeze(dim=0).detach().cpu(), -1),\n",
    "                             np.expand_dims(outputs.end_logits.softmax(dim=1)[batch_idx].unsqueeze(dim=0).detach().cpu(), 1)).squeeze(axis=0)\n",
    "\n",
    "        answ_confidence.append(\n",
    "            conf_mat[start_idxs[batch_idx], end_idxs[batch_idx]].item()\n",
    "        )\n",
    "\n",
    "    return answ_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutLMv3_hy:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.apply_ocr = config['apply_ocr']\n",
    "        self.processor = LayoutLMv3Processor.from_pretrained(config['model_weights'], apply_ocr=config['apply_ocr'])  # Check that this do not fuck up the code.\n",
    "        # self.processor = LayoutLMv3Processor.from_pretrained(config['model_weights'], apply_ocr=False)  # Check that this do not fuck up the code.\n",
    "        self.model = LayoutLMv3ForQuestionAnswering.from_pretrained(config['model_weights'])\n",
    "        self.ignore_index = 9999  # 0\n",
    "\n",
    "    # def parallelize(self):\n",
    "    #     self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, batch, return_pred_answer=False):\n",
    "\n",
    "        # bs = len(batch['question_id'])\n",
    "        question = batch['questions']\n",
    "        context = batch['contexts']\n",
    "        answers = batch['answers']\n",
    "        images = batch['images']\n",
    "\n",
    "        boxes = [(bbox * 1000).astype(int) for bbox in batch['boxes']]  # Scale boxes 0->1 to 0-->1000.\n",
    "        \n",
    "        if self.apply_ocr:\n",
    "            encoding = self.processor(images, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "        else:\n",
    "            encoding = self.processor(images, question, batch[\"words\"], boxes=boxes, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "\n",
    "        start_pos, end_pos = self.get_start_end_idx(encoding, context, answers)\n",
    "        outputs = self.model(**encoding, start_positions=start_pos, end_positions=end_pos)\n",
    "        pred_answers, answ_confidence = self.get_answer_from_model_output(encoding.input_ids, outputs) if return_pred_answer else None\n",
    "\n",
    "        return outputs, pred_answers, answ_confidence\n",
    "\n",
    "    def get_concat_v_multi_resize(self, im_list, resample=Image.BICUBIC):\n",
    "        min_width = min(im.width for im in im_list)\n",
    "        im_list_resize = [im.resize((min_width, int(im.height * min_width / im.width)), resample=resample) for im in im_list]\n",
    "\n",
    "        # Fix equal height for all images (breaks the aspect ratio).\n",
    "        heights = [im.height for im in im_list]\n",
    "        im_list_resize = [im.resize((im.height, max(heights)), resample=resample) for im in im_list_resize]\n",
    "\n",
    "        total_height = sum(im.height for im in im_list_resize)\n",
    "        dst = Image.new('RGB', (min_width, total_height))\n",
    "        pos_y = 0\n",
    "        for im in im_list_resize:\n",
    "            dst.paste(im, (0, pos_y))\n",
    "            pos_y += im.height\n",
    "        return dst\n",
    "\n",
    "    def get_start_end_idx(self, encoding, context, answers):\n",
    "        pos_idx = []\n",
    "        for batch_idx in range(len(encoding.input_ids)):\n",
    "            answer_pos = []\n",
    "            for answer in answers[batch_idx]:\n",
    "                encoded_answer = [token for token in self.processor.tokenizer.encode([answer], boxes=[0, 0, 0, 0]) if token not in self.processor.tokenizer.all_special_ids]\n",
    "                answer_tokens_length = len(encoded_answer)\n",
    "\n",
    "                for token_pos in range(len(encoding.input_ids[batch_idx])):\n",
    "                    if encoding.input_ids[batch_idx][token_pos: token_pos+answer_tokens_length].tolist() == encoded_answer:\n",
    "                        answer_pos.append([token_pos, token_pos + answer_tokens_length-1])\n",
    "\n",
    "            if len(answer_pos) == 0:\n",
    "                pos_idx.append([self.ignore_index, self.ignore_index])\n",
    "\n",
    "            else:\n",
    "                answer_pos = random.choice(answer_pos)  # To add variability, pick a random correct span.\n",
    "                pos_idx.append(answer_pos)\n",
    "\n",
    "        start_idxs = torch.LongTensor([idx[0] for idx in pos_idx]).to(self.model.device)\n",
    "        end_idxs = torch.LongTensor([idx[1] for idx in pos_idx]).to(self.model.device)\n",
    "\n",
    "        return start_idxs, end_idxs\n",
    "\n",
    "    def get_answer_from_model_output(self, input_tokens, outputs):\n",
    "        predicted_start_idxs = torch.argmax(outputs.start_logits, axis=1)\n",
    "        predicted_end_idxs = torch.argmax(outputs.end_logits, axis=1)\n",
    "\n",
    "        predicted_answers = [self.processor.tokenizer.decode(input_tokens[batch_idx][predicted_start_idxs[batch_idx]: predicted_end_idxs[batch_idx]+1], skip_special_tokens=True).strip() for batch_idx in range(len(input_tokens))]\n",
    "        # answers_conf = ((outputs.start_logits.max(dim=1).values + outputs.end_logits.max(dim=1).values) / 2).tolist()\n",
    "\n",
    "        start_logits = outputs.start_logits.softmax(dim=1).detach().cpu()\n",
    "        end_logits = outputs.end_logits.softmax(dim=1).detach().cpu()\n",
    "        answ_confidence = []\n",
    "        for batch_idx in range(len(input_tokens)):\n",
    "            conf_mat = np.matmul(np.expand_dims(start_logits[batch_idx].unsqueeze(dim=0), -1),\n",
    "                                 np.expand_dims(end_logits[batch_idx].unsqueeze(dim=0), 1)).squeeze(axis=0)\n",
    "\n",
    "            answ_confidence.append(\n",
    "                conf_mat[predicted_start_idxs[batch_idx], predicted_end_idxs[batch_idx]].item()\n",
    "            )\n",
    "\n",
    "        answ_confidence = get_extractive_confidence(outputs)\n",
    "\n",
    "        return predicted_answers, answ_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    # \"model\": \"hy\",\n",
    "    \"dataset\": \"infographics\",\n",
    "    \"eval_start\": True,\n",
    "    \"no_eval_start\": False,\n",
    "    \"page_retrieval\": None,\n",
    "    \"batch_size\": None,\n",
    "    \"max_sequence_length\": None,\n",
    "    \"seed\": 42,\n",
    "    \"save_dir\": \"saving_dir/\",\n",
    "    \"apply_ocr\": False,\n",
    "    \"data_parallel\": False,\n",
    "    \"no_data_parallel\": False,\n",
    "    \"model_name\": \"hy\",\n",
    "    \"model_weights\": \"microsoft/layoutlmv3-base\",\n",
    "    \"device\": \"cuda\",\n",
    "    # \"training_parameters\": {\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"train_epochs\": 10,\n",
    "    \"warmup_iterations\": 5,\n",
    "    # },\n",
    "    \"dataset_name\": \"infographicVQA\",\n",
    "    # \"imdb_dir\": \"./task3/imdb\",\n",
    "    # \"images_dir\": \"./task3/images\",\n",
    "    \"imdb_dir\": \"./Task3_test/imdb\",\n",
    "    \"images_dir\": \"./Task3_test/images\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['qa_outputs.dense.bias', 'qa_outputs.dense.weight', 'qa_outputs.out_proj.bias', 'qa_outputs.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForQuestionAnswering(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = args_dict\n",
    "# config.pop('model')\n",
    "model_name = config['model_name'].lower()\n",
    "if 'save_dir' in config:\n",
    "    if not config['save_dir'].endswith('/'):\n",
    "        config['save_dir'] = config['save_dir'] + '/'\n",
    "\n",
    "    if not os.path.exists(config['save_dir']):\n",
    "        os.makedirs(config['save_dir'])\n",
    "\n",
    "# if 'seed' not in config:\n",
    "#     print(\"Seed not specified. Setting default seed to '{:d}'\".format(42))\n",
    "#     config['seed'] = 42\n",
    "\n",
    "model = LayoutLMv3_hy(config)\n",
    "\n",
    "if config['device'] == 'cuda' and config['data_parallel'] and torch.cuda.device_count() > 1:\n",
    "    model.parallelize()\n",
    "\n",
    "model.model.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbigchoi3449\u001b[0m (\u001b[33mlevel2-cv-10-detection\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>saving_dir/wandb/run-20240313_192137-jo1t1m0v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~/runs/jo1t1m0v' target=\"_blank\">hy__2024.03.13_19.21.35</a></strong> to <a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~/runs/jo1t1m0v' target=\"_blank\">https://wandb.ai/level2-cv-10-detection/Hyunyoung%20in%20the%20house%20motherfuckers~/runs/jo1t1m0v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hy: microsoft/layoutlmv3-base \n",
      "{\n",
      "\tDataset: infographicVQA\n",
      "\tVisual Encoder: -\n",
      "\tBatch size: 20\n",
      "\tMax. Seq. Length: None\n",
      "\tlr: 0.0001\n",
      "\tseed: 42\n",
      "}\n",
      "\n",
      "Model parameters: 125919106 - Trainable: 125919106 (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chy/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/chy/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:720: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.tensor(value)\n",
      "/home/chy/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -1: Accuracy 0.00     ANLS 0.0000    Retrieval precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:04<00:00,  8.10s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuracy 0.00     ANLS 0.0000    Retrieval precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:06<00:00,  8.27s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy 0.00     ANLS 0.0000    Retrieval precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:05<00:00,  8.20s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Accuracy 0.00     ANLS 0.0000    Retrieval precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:05<00:00,  8.14s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Accuracy 0.00     ANLS 0.0000    Retrieval precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [01:03<00:21, 10.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     33\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m=\u001b[39m epoch_ix\n\u001b[0;32m---> 34\u001b[0m     train_epoch(train_data_loader, model, optimizer, lr_scheduler, evaluator, logger, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m     accuracy, anls, ret_prec, _, _ \u001b[38;5;241m=\u001b[39m evaluate(val_data_loader, model, evaluator, return_scores_by_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_pred_answers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m     is_updated \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mupdate_global_metrics(accuracy, anls, epoch_ix)\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data_loader, model, optimizer, lr_scheduler, evaluator, logger, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# loss = outputs.loss + outputs.ret_loss if hasattr(outputs, 'ret_loss') else outputs.loss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

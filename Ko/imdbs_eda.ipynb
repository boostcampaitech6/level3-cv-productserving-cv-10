{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .npy 파일 로드 및 데이터 구조 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 4567 column 37 (char 156427)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m json_file \u001b[38;5;129;01min\u001b[39;00m json_files:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 파일을 열어 JSON 데이터를 로드합니다.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 14\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 동일한 파일에 JSON 데이터를 다시 저장합니다.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 4567 column 37 (char 156427)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "ocr_dir = '../data/sp-vqa/ocr'\n",
    "\n",
    "# ocr_dir 디렉토리 안의 모든 .json 파일을 찾습니다.\n",
    "json_files = glob.glob(os.path.join(ocr_dir, '*.json'))\n",
    "\n",
    "# 모든 JSON 파일에 대해 반복합니다.\n",
    "for json_file in json_files:\n",
    "    # 파일을 열어 JSON 데이터를 로드합니다.\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 동일한 파일에 JSON 데이터를 다시 저장합니다.\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/sp-vqa/ocr/ffbf0023_4.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecognitionResults\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/sp-vqa/ocr/ffbf0023_4.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(data)\n",
    "print(data['recognitionResults'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'numpy.ndarray'>\n",
      "Data shape: (39464,)\n",
      "Data size: 39464\n",
      "First element type: <class 'dict'>\n",
      "Keys in the first dictionary element: dict_keys(['creation_time', 'version', 'dataset_type', 'has_answer'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# .npy 파일 로드\n",
    "# data = np.load('/home/ges/level3-cv-productserving-cv-10/data/infographics/imdb/new_imdb_train.npy', allow_pickle=True)\n",
    "data = np.load('/home/ges/level3-cv-productserving-cv-10/data/sp-vqa/ibmb/new_imdb_train.npy', allow_pickle=True)\n",
    "\n",
    "# 데이터 타입 출력\n",
    "print(f\"Data type: {type(data)}\")\n",
    "\n",
    "# 데이터의 전체 구조 확인 (예: shape, size)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data size: {data.size}\")\n",
    "\n",
    "# 데이터의 첫 번째 요소 타입 확인\n",
    "print(f\"First element type: {type(data[0])}\")\n",
    "\n",
    "# 데이터의 첫 번째 요소 내용 확인 (예시로, 배열이나 리스트일 경우)\n",
    "if isinstance(data[0], (np.ndarray, list)):\n",
    "    print(f\"First element content: {data[0]}\")\n",
    "\n",
    "# 더 구체적인 데이터 구조 및 라벨링 확인\n",
    "# 예: 첫 번째 요소가 딕셔너리라면, 키를 확인하여 어떤 라벨/필드가 있는지 확인\n",
    "if isinstance(data[1], dict):\n",
    "    print(\"Keys in the first dictionary element:\", data[0].keys())\n",
    "    \n",
    "    \n",
    "\n",
    "# for item in data :\n",
    "#     print(item.keys())\n",
    "# if isinstance(data[0], dict):\n",
    "#     print(\"Keys in the first dictionary element:\", data[0].values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'image_id', 'image_classes', 'extra_info', 'image_width', 'image_height', 'question_tokens', 'question_id', 'set_name', 'image_name', 'image_path', 'feature_path', 'ocr_tokens', 'ocr_info', 'ocr_normalized_boxes', 'obj_normalized_boxes', 'answers', 'valid_answers'])\n",
      "['b&w', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', '.', '.', '.', '.', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'cc:', 't.f.', 'riehl', 'from:', 'c.', 'j.', 'cook', 'date:', 'may', '8.', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'group', 'is', 'to', 'develop', 'marketable', 'novel', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'ncorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'conventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'one', '.multi-plug', 'to', 'build', 'your', 'own', 'cigarette.', '.switchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', 'future', 'smoking.', 'cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', '.short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', '.', 'extremely', 'fast', 'buming', 'cigarette.', '-.', '..\".', '\"', '.', '.', 'novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'in', 'tobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', '.', '.', '.', '.', 'higher', 'basis', 'weight', 'paper;', 'potential', 'reduction', 'in', 'tobacco', 'weight.', '.', 'more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '.', 'colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'all', 'white', 'cigarettes.', '499150498', 'source:', 'https://www.industrydocuments.ucsf.edu/docs/mxcj0037']\n"
     ]
    }
   ],
   "source": [
    "print(data[1].keys())\n",
    "# print(data[8]['question'])\n",
    "# import re\n",
    "# cleaned_sentence = re.sub(r'[^\\w\\s]', '', data[8]['question'])\n",
    "\n",
    "# 공백 기준으로 토큰 나누기\n",
    "# tokens = cleaned_sentence.split()\n",
    "\n",
    "# print(data[4]['ocr_tokens'])\n",
    "\n",
    "# print(data[0])\n",
    "# print(len(data[1]['ocr_tokens']))\n",
    "# print(data[1]['ocr_tokens'])\n",
    "# print(data[1]['question'])\n",
    "# print(data[1]['question_id'])\n",
    "# print(data[1]['question_tokens']) # 이거 안드러가있음\n",
    "# print(len(data[1]['ocr_normalized_boxes']))\n",
    "# print(data[2231]['valid_answers'])\n",
    "# print(data[2231]['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded',\n",
       " 'recognitionResults': [{'page': 1,\n",
       "   'clockwiseOrientation': 359.79,\n",
       "   'width': 1695,\n",
       "   'height': 2025,\n",
       "   'unit': 'pixel',\n",
       "   'lines': [{'boundingBox': [576, 30, 992, 26, 993, 126, 577, 130],\n",
       "     'text': 'Confidential',\n",
       "     'words': [{'boundingBox': [586, 30, 993, 36, 994, 119, 584, 131],\n",
       "       'text': 'Confidential',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [1081, 34, 1124, 31, 1125, 53, 1082, 56],\n",
       "     'text': '.. ..',\n",
       "     'words': [{'boundingBox': [1080, 34, 1097, 32, 1099, 54, 1081, 55],\n",
       "       'text': '..',\n",
       "       'confidence': 'Low'},\n",
       "      {'boundingBox': [1102, 32, 1122, 31, 1124, 53, 1103, 54],\n",
       "       'text': '..'}]},\n",
       "    {'boundingBox': [695, 202, 994, 204, 994, 236, 694, 234],\n",
       "     'text': 'RJRT PR APPROVAL',\n",
       "     'words': [{'boundingBox': [699, 203, 779, 204, 779, 234, 699, 234],\n",
       "       'text': 'RJRT',\n",
       "       'confidence': 'Low'},\n",
       "      {'boundingBox': [793, 204, 832, 204, 832, 234, 793, 234], 'text': 'PR'},\n",
       "      {'boundingBox': [848, 204, 995, 204, 995, 237, 848, 234],\n",
       "       'text': 'APPROVAL'}]},\n",
       "    {'boundingBox': [254, 295, 343, 294, 344, 322, 255, 323],\n",
       "     'text': 'DATE :',\n",
       "     'words': [{'boundingBox': [259, 296, 330, 295, 331, 322, 259, 323],\n",
       "       'text': 'DATE'},\n",
       "      {'boundingBox': [335, 295, 343, 295, 344, 322, 337, 322], 'text': ':'}]},\n",
       "    {'boundingBox': [396, 262, 561, 262, 559, 333, 398, 327],\n",
       "     'text': '1/8/13',\n",
       "     'words': [{'boundingBox': [400, 258, 556, 261, 555, 332, 399, 329],\n",
       "       'text': '1/8/13',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [399, 331, 644, 330, 645, 410, 400, 412],\n",
       "     'text': 'Ru alAs',\n",
       "     'words': [{'boundingBox': [426, 334, 523, 331, 522, 410, 426, 412],\n",
       "       'text': 'Ru'},\n",
       "      {'boundingBox': [548, 331, 644, 333, 643, 413, 547, 411],\n",
       "       'text': 'alAs',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [249, 410, 658, 409, 659, 448, 250, 448],\n",
       "     'text': 'PROPOSED RELEASE DATE:',\n",
       "     'words': [{'boundingBox': [259, 416, 403, 414, 405, 445, 260, 444],\n",
       "       'text': 'PROPOSED'},\n",
       "      {'boundingBox': [422, 413, 551, 412, 553, 447, 423, 445],\n",
       "       'text': 'RELEASE'},\n",
       "      {'boundingBox': [570, 411, 657, 410, 659, 449, 572, 447],\n",
       "       'text': 'DATE:',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [673, 400, 970, 385, 973, 449, 677, 464],\n",
       "     'text': 'for response',\n",
       "     'words': [{'boundingBox': [688, 400, 764, 398, 766, 457, 690, 464],\n",
       "       'text': 'for',\n",
       "       'confidence': 'Low'},\n",
       "      {'boundingBox': [777, 398, 972, 393, 972, 445, 778, 456],\n",
       "       'text': 'response',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [257, 472, 530, 471, 531, 504, 258, 505],\n",
       "     'text': 'FOR RELEASE TO:',\n",
       "     'words': [{'boundingBox': [260, 474, 314, 473, 315, 505, 260, 505],\n",
       "       'text': 'FOR'},\n",
       "      {'boundingBox': [330, 473, 462, 472, 462, 506, 331, 505],\n",
       "       'text': 'RELEASE',\n",
       "       'confidence': 'Low'},\n",
       "      {'boundingBox': [480, 472, 531, 473, 530, 505, 480, 505],\n",
       "       'text': 'TO:',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [252, 529, 411, 530, 410, 565, 251, 564],\n",
       "     'text': 'CONTACT:',\n",
       "     'words': [{'boundingBox': [258, 531, 409, 530, 412, 565, 259, 564],\n",
       "       'text': 'CONTACT:'}]},\n",
       "    {'boundingBox': [429, 521, 663, 511, 666, 568, 432, 578],\n",
       "     'text': 'P. CARTER',\n",
       "     'words': [{'boundingBox': [445, 521, 490, 519, 492, 577, 447, 578],\n",
       "       'text': 'P.'},\n",
       "      {'boundingBox': [501, 519, 658, 513, 660, 568, 503, 577],\n",
       "       'text': 'CARTER'}]},\n",
       "    {'boundingBox': [256, 653, 410, 654, 410, 682, 255, 681],\n",
       "     'text': 'ROUTE TO',\n",
       "     'words': [{'boundingBox': [258, 656, 347, 656, 349, 683, 259, 681],\n",
       "       'text': 'ROUTE'},\n",
       "      {'boundingBox': [368, 655, 401, 654, 403, 682, 369, 683],\n",
       "       'text': 'TO',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [1037, 711, 1194, 715, 1193, 748, 1036, 744],\n",
       "     'text': 'Initials',\n",
       "     'words': [{'boundingBox': [1048, 713, 1194, 715, 1193, 749, 1047, 745],\n",
       "       'text': 'Initials',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [1357, 713, 1436, 718, 1434, 751, 1356, 745],\n",
       "     'text': 'pate',\n",
       "     'words': [{'boundingBox': [1359, 712, 1433, 717, 1431, 750, 1356, 745],\n",
       "       'text': 'pate',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [253, 773, 481, 775, 480, 807, 252, 806],\n",
       "     'text': 'Peggy Carter',\n",
       "     'words': [{'boundingBox': [257, 774, 348, 777, 348, 807, 257, 804],\n",
       "       'text': 'Peggy',\n",
       "       'confidence': 'Low'},\n",
       "      {'boundingBox': [366, 778, 478, 776, 478, 804, 366, 807],\n",
       "       'text': 'Carter'}]},\n",
       "    {'boundingBox': [1019, 756, 1129, 755, 1129, 804, 1017, 808],\n",
       "     'text': 'Ac',\n",
       "     'words': [{'boundingBox': [1053, 755, 1127, 753, 1129, 804, 1054, 806],\n",
       "       'text': 'Ac',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [250, 833, 458, 837, 457, 866, 249, 862],\n",
       "     'text': 'Maura Payne',\n",
       "     'words': [{'boundingBox': [254, 835, 349, 838, 349, 864, 254, 863],\n",
       "       'text': 'Maura'},\n",
       "      {'boundingBox': [367, 838, 456, 837, 457, 866, 367, 865],\n",
       "       'text': 'Payne',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [254, 890, 473, 892, 472, 925, 253, 924],\n",
       "     'text': 'David Fishel',\n",
       "     'words': [{'boundingBox': [254, 893, 349, 894, 351, 925, 256, 923],\n",
       "       'text': 'David'},\n",
       "      {'boundingBox': [365, 894, 472, 892, 473, 925, 367, 925],\n",
       "       'text': 'Fishel',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [225, 943, 566, 931, 568, 979, 227, 991],\n",
       "     'text': 'Tom GRISCom',\n",
       "     'words': [{'boundingBox': [243, 944, 319, 940, 320, 989, 244, 989],\n",
       "       'text': 'Tom'},\n",
       "      {'boundingBox': [355, 939, 562, 935, 563, 977, 355, 988],\n",
       "       'text': 'GRISCom',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [236, 1003, 543, 983, 546, 1030, 239, 1050],\n",
       "     'text': 'Diane Barrows',\n",
       "     'words': [{'boundingBox': [247, 1003, 370, 994, 371, 1043, 248, 1049],\n",
       "       'text': 'Diane'},\n",
       "      {'boundingBox': [391, 993, 544, 985, 546, 1030, 392, 1042],\n",
       "       'text': 'Barrows',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [244, 1053, 514, 1034, 518, 1087, 248, 1105],\n",
       "     'text': 'Ed Blackmer',\n",
       "     'words': [{'boundingBox': [249, 1056, 310, 1050, 313, 1101, 252, 1106],\n",
       "       'text': 'Ed'},\n",
       "      {'boundingBox': [336, 1047, 511, 1038, 516, 1088, 340, 1099],\n",
       "       'text': 'Blackmer'}]},\n",
       "    {'boundingBox': [241, 1115, 500, 1088, 506, 1140, 247, 1167],\n",
       "     'text': 'Tow Rucker',\n",
       "     'words': [{'boundingBox': [256, 1115, 330, 1107, 334, 1159, 260, 1165],\n",
       "       'text': 'Tow'},\n",
       "      {'boundingBox': [371, 1103, 500, 1089, 504, 1141, 376, 1155],\n",
       "       'text': 'Rucker',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [1029, 1119, 1124, 1112, 1128, 1172, 1038, 1185],\n",
       "     'text': 'TR',\n",
       "     'words': [{'boundingBox': [1037,\n",
       "        1118,\n",
       "        1120,\n",
       "        1109,\n",
       "        1127,\n",
       "        1174,\n",
       "        1044,\n",
       "        1183],\n",
       "       'text': 'TR',\n",
       "       'confidence': 'Low'}]},\n",
       "    {'boundingBox': [393, 1252, 1283, 1253, 1282, 1289, 392, 1288],\n",
       "     'text': 'Return to Peggy Carter, PR, 16 Reynolds Building',\n",
       "     'words': [{'boundingBox': [399, 1254, 512, 1254, 511, 1288, 398, 1287],\n",
       "       'text': 'Return'},\n",
       "      {'boundingBox': [529, 1254, 568, 1254, 568, 1288, 528, 1288],\n",
       "       'text': 'to'},\n",
       "      {'boundingBox': [584, 1254, 679, 1254, 679, 1289, 583, 1288],\n",
       "       'text': 'Peggy'},\n",
       "      {'boundingBox': [693, 1254, 830, 1254, 830, 1289, 692, 1289],\n",
       "       'text': 'Carter,'},\n",
       "      {'boundingBox': [841, 1254, 902, 1254, 902, 1289, 840, 1289],\n",
       "       'text': 'PR,'},\n",
       "      {'boundingBox': [917, 1254, 956, 1254, 956, 1290, 917, 1290],\n",
       "       'text': '16'},\n",
       "      {'boundingBox': [969, 1254, 1121, 1254, 1122, 1290, 969, 1290],\n",
       "       'text': 'Reynolds'},\n",
       "      {'boundingBox': [1135, 1254, 1280, 1253, 1281, 1289, 1135, 1290],\n",
       "       'text': 'Building'}]},\n",
       "    {'boundingBox': [1527, 1536, 1500, 1721, 1468, 1716, 1495, 1532],\n",
       "     'text': '51142 3977',\n",
       "     'words': [{'boundingBox': [1527,\n",
       "        1542,\n",
       "        1514,\n",
       "        1630,\n",
       "        1483,\n",
       "        1623,\n",
       "        1495,\n",
       "        1536],\n",
       "       'text': '51142'},\n",
       "      {'boundingBox': [1511, 1648, 1499, 1719, 1470, 1712, 1481, 1641],\n",
       "       'text': '3977'}]},\n",
       "    {'boundingBox': [1496, 1917, 1529, 1922, 1527, 1940, 1495, 1934],\n",
       "     'text': '. .',\n",
       "     'words': [{'boundingBox': [1499,\n",
       "        1916,\n",
       "        1504,\n",
       "        1917,\n",
       "        1501,\n",
       "        1935,\n",
       "        1496,\n",
       "        1934],\n",
       "       'text': '.'},\n",
       "      {'boundingBox': [1507, 1918, 1519, 1920, 1516, 1938, 1504, 1935],\n",
       "       'text': '.'}]},\n",
       "    {'boundingBox': [373, 1983, 1315, 1982, 1316, 2017, 374, 2018],\n",
       "     'text': 'Source: https://www.industrydocuments.ucsf.edu/docs/xnb10037',\n",
       "     'words': [{'boundingBox': [378, 1985, 492, 1986, 493, 2018, 379, 2017],\n",
       "       'text': 'Source:'},\n",
       "      {'boundingBox': [498, 1986, 1314, 1983, 1315, 2018, 499, 2018],\n",
       "       'text': 'https://www.industrydocuments.ucsf.edu/docs/xnb10037',\n",
       "       'confidence': 'Low'}]}]}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('../data/sp-vqa/ocr/xnbl0037_1.json','r') as f:\n",
    "    a = json.load(f)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'confidential'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['ocr_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34454277, 0.01481481, 0.58643067, 0.06469136], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['ocr_normalized_boxes'][0]\n",
    "# len(data)\n",
    "# data[1]\n",
    "\n",
    "# import json\n",
    "\n",
    "\n",
    "# with open('asdsadasd.json', 'w', encoding='utf-8') as ff:\n",
    "#     json.dump(data[1], ff, indent=4)\n",
    "# data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3445427728613569, 0.014814814814814815, 0.5864306784660767, 0.06469135802469136]\n"
     ]
    }
   ],
   "source": [
    "original_bbox = [586, 30, 993, 36, 994, 119, 584, 131]\n",
    "\n",
    "# 이미지의 너비와 높이\n",
    "image_width = 1695\n",
    "image_height = 2025\n",
    "\n",
    "# 최소/최대 좌표 계산\n",
    "min_x = min(original_bbox[::2])  # x 좌표는 짝수 인덱스\n",
    "max_x = max(original_bbox[::2])\n",
    "min_y = min(original_bbox[1::2])  # y 좌표는 홀수 인덱스\n",
    "max_y = max(original_bbox[1::2])\n",
    "\n",
    "# 정규화\n",
    "normalized_bbox = [\n",
    "    min_x / image_width, min_y / image_height,\n",
    "    max_x / image_width, max_y / image_height\n",
    "]\n",
    "\n",
    "print(normalized_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation_time: <class 'float'>\n",
      "version: <class 'float'>\n",
      "dataset_type: <class 'str'>\n",
      "has_answer: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "# 예: 중첩된 구조 탐색\n",
    "if isinstance(data[0], dict):\n",
    "    for key, value in data[0].items():\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "\n",
    "        # 딕셔너리 또는 리스트 내부의 더 세부적인 정보 확인\n",
    "        if isinstance(value, (dict, list)):\n",
    "            print(f\"Sample from '{key}': {value[:1]}\")  # 첫 번째 요소 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
